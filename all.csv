ProblemType,Problem,Solution
Sequences and Limits,Prove that $n \sin(2n! e \pi)$ converges to $2\pi$ as $n \to \infty$.,"Let $r_n$ and $\epsilon_n$ be the integral and fractional parts of the number $n! e$ respectively. Using the expansion
$$
e = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \cdots,
$$
we have
$$
\begin{cases}
r_n = n! \left( 1 + \frac{1}{1!} + \frac{1}{2!} + \cdots + \frac{1}{n!} \right) \\
\epsilon_n = \frac{1}{n+1} + \frac{1}{(n+1)(n+2)} + \cdots,
\end{cases}
$$
since
$$
\frac{1}{n+1} < \epsilon_n < \frac{1}{n+1} + \frac{1}{(n+1)^2} + \cdots = \frac{1}{n}.
$$

Thus $\sin(2n! e \pi) = \sin(2\pi \epsilon_n)$. Note that this implies the irrationality of $e$.

Since $n \epsilon_n$ converges to $1$ as $n \to \infty$, we have
$$
\lim_{n \to \infty} n \sin(2\pi \epsilon_n) = \lim_{n \to \infty} \frac{\sin(2\pi \epsilon_n)}{\epsilon_n} = 2\pi.
$$

Hence $n \sin(2n! e \pi)$ converges to $2\pi$ as $n \to \infty$."
Sequences and Limits,"Prove that the sequence
$$
\left( \frac{1}{n} \right)^n + \left( \frac{2}{n} \right)^n + \cdots + \left( \frac{n}{n} \right)^n
$$
converges to $e / (e - 1)$ as $n \to \infty$.","Let $r_n$ and $\epsilon_n$ be the integral and fractional parts of the number $n! e$ respectively. Using the expansion
$$
e = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \cdots,
$$
we have
$$
\begin{cases}
r_n = n! \left( 1 + \frac{1}{1!} + \frac{1}{2!} + \cdots + \frac{1}{n!} \right) \\
\epsilon_n = \frac{1}{n+1} + \frac{1}{(n+1)(n+2)} + \cdots,
\end{cases}
$$
since
$$
\frac{1}{n+1} < \epsilon_n < \frac{1}{n+1} + \frac{1}{(n+1)^2} + \cdots = \frac{1}{n}.
$$

Thus $\sin(2n! e \pi) = \sin(2\pi \epsilon_n)$. Note that this implies the irrationality of $e$.

Since $n \epsilon_n$ converges to $1$ as $n \to \infty$, we have
$$
\lim_{n \to \infty} n \sin(2\pi \epsilon_n) = \lim_{n \to \infty} \frac{\sin(2\pi \epsilon_n)}{\epsilon_n} = 2\pi.
$$

Hence $n \sin(2n! e \pi)$ converges to $2\pi$ as $n \to \infty$."
Sequences and Limits,"Suppose that $a_n$ and $b_n$ converge to $\alpha$ and $\beta$ as $n \to \infty$ respectively. Show that the sequence
$$
\frac{a_0 b_n + a_1 b_{n-1} + \cdots + a_n b_0}{n}
$$
converges to $\alpha \beta$ as $n \to \infty$.","Let $M$ be an upper bound of the two convergent sequences $|a_n|$ and $|b_n|$. For any $\epsilon > 0$ we can take a positive integer $N$ satisfying $|a_n - \alpha| < \epsilon$ and $|b_n - \beta| < \epsilon$ for all integers $n$ greater than $N$. If $n$ is greater than $N^2$, then
$$
|a_k b_{n-k} - \alpha \beta| \leq |(a_k - \alpha) b_{n-k} + \alpha (b_{n-k} - \beta)| 
\leq (M + |\alpha|) \epsilon
$$
for any integer $k$ in the interval $\left[\sqrt{n}, n - \sqrt{n}\right]$. Therefore
$$
\left| \frac{1}{n} \sum_{k=0}^n a_k b_{n-k} - \alpha \beta \right| 
\leq \frac{1}{n} \sum_{\sqrt{n} \leq k \leq n - \sqrt{n}} |a_k b_{n-k} - \alpha \beta|
+ 2 \left(|\alpha \beta| + M^2 \right) \frac{\lfloor \sqrt{n} \rfloor + 1}{n}
$$
$$
\leq (M + |\alpha|) \epsilon + 2 \left(|\alpha \beta| + M^2\right) \frac{\sqrt{n} + 1}{n}.
$$

We can take $n$ so large that the last expression is less than $(M + |\alpha| + 1)\epsilon$."
Sequences and Limits,"Suppose that $\{a_n\}_{n \geq 0}$ is a non-negative sequence satisfying
$$
a_{m+n} \leq a_m + a_n + C
$$
for all positive integers $m, n$ and some non-negative constant $C$. Show that $a_n / n$ converges as $n \to \infty$.","For an arbitrary fixed positive integer $k$ we put $n = qk + r$ with $0 \leq r < k$. Since $a_n = a_{qk+r} \leq q(a_k + C) + a_r$, we have
$$
\frac{a_n}{n} \leq \frac{a_k + C}{k} + \frac{a_r}{n}.
$$
Taking the limit as $n \to \infty$, we get
$$
\limsup_{n \to \infty} \frac{a_n}{n} \leq \frac{a_k + C}{k}.
$$
The sequence $a_n / n$ is therefore bounded. Since $k$ is arbitrary, we may conclude that
$$
\limsup_{n \to \infty} \frac{a_n}{n} \leq \liminf_{k \to \infty} \frac{a_k}{k},
$$
which means the convergence of $a_n / n$. 
"
Sequences and Limits,"For any positive sequence $\{a_n\}_{n \geq 1}$, show that
$$
\left( \frac{a_1 + a_{n+1}}{a_n} \right)^n > e
$$
for infinitely many $n$'s, where $e$ is the base of the natural logarithm. Prove moreover that the constant $e$ on the right-hand side cannot in general be replaced by any larger number.","Without loss of generality we may put $a_1 = 1$. Suppose, contrary to the conclusion, that there is an integer $N$ satisfying
$$
\left(\frac{1 + a_{n+1}}{a_n}\right)^n \leq e
$$
for all $n \geq N$. Put
$$
s_{j,k} = \exp\left(\frac{1}{j} + \cdots + \frac{1}{k}\right)
$$
for any integers $j \leq k$. Since $0 < a_{n+1} \leq e^{1/n} a_n - 1$, we get successively
$$
\begin{cases}
0 < a_{n+1} \leq s_{n,n} a_n - 1, \\
0 < a_{n+2} \leq s_{n,n+1} a_n - s_{n+1,n+1} - 1, \\
\vdots \\
0 < a_{n+k+1} \leq s_{n,n+k} a_n - s_{n+1,n+k} - \cdots - s_{n+k,n+k} - 1
\end{cases}
$$
for any non-negative integer $k$. Hence it follows that
$$
a_n > \frac{1}{s_{n,n}} + \frac{1}{s_{n,n+1}} + \cdots + \frac{1}{s_{n,n+k}}.
$$

On the other hand, using the inequality
$$
\frac{1}{s_{n,n+j}} > \exp\left(-\int_{n-1}^{n+j} \frac{dx}{x}\right) = \frac{n-1}{n+j},
$$
we get
$$
a_n > \sum_{j=0}^k \frac{n-1}{n+j},
$$
which is a contradiction, since the right-hand side diverges to $\infty$ as $k \to \infty$.

To see that the bound $e$ cannot be replaced by any larger number, consider the case $a_n = n \log n$ for $n \geq 2$. Then
$$
\left(\frac{a_1 + (n+1)\log(n+1)}{n \log n}\right)^n = \exp\left(n \log\left(1 + \frac{1}{n} + O\left(\frac{1}{n \log n}\right)\right)\right)
$$
$$
= \exp\left(1 + O\left(\frac{1}{\log n}\right)\right),
$$
which converges to $e$ as $n \to \infty$.
"
Sequences and Limits,"For any $0 < \theta < \pi$ and any positive integer $n$, show the inequality
$$
\sin\theta + \frac{\sin 2\theta}{2} + \cdots + \frac{\sin n\theta}{n} > 0.
$$","Denote by $s_n(\theta)$ the left-hand side of the inequality to be shown. Write $\theta$ for $2\vartheta$ for brevity. Since
$$
s_n'(\theta) = \Re \left(e^{i\theta} + e^{2i\theta} + \cdots + e^{ni\theta} \right)
= \frac{\cos(n+1)\vartheta \sin n\vartheta}{\sin\vartheta},
$$
we obtain the candidates for extreme points of $s_n(\theta)$ on the interval $(0, \pi]$ by solving the equations $\cos(n+1)\vartheta = 0$ and $\sin n\vartheta = 0$, as follows:
$$
\frac{\pi}{n+1}, \frac{2\pi}{n}, \frac{3\pi}{n+1}, \frac{4\pi}{n}, \dots
$$
where the last two candidates are $(n-1)\pi/(n+1)$ and $\pi$ if $n$ is even, and $(n-1)\pi/n$ and $n\pi/(n+1)$ if $n$ is odd. In any case $s_n'(\theta)$ vanishes at least at $n$ points in the interval $(0, \pi)$.

Since $s_n'(\theta)$ can be expressed as a polynomial in $\cos\theta$ of degree $n$ and $\cos\theta$ maps the interval $[0, \pi]$ onto $[-1, 1]$ homeomorphically, this polynomial possesses at most $n$ real roots in $[-1, 1]$. Therefore all these roots must be simple and give the actual extreme points of $s_n(\theta)$ except for $\theta = \pi$. Clearly $s_n(\theta)$ is positive in the right neighborhood of the origin, and the maximal and minimal points stand in line alternately from left to right. Thus $s_n(\theta)$ attains its minimal values at the points $2\ell\pi/n \in (0, \pi)$ when $n \geq 3$. In the cases $n=1$ and $n=2$, however, $s_n(\theta)$ has no minimal points in $(0, \pi)$.

Now we will show that $s_n(\theta)$ is positive on the interval $(0, \pi)$ by induction on $n$. This is clear for $n=1$ and $n=2$ since $s_1(\theta) = \sin\theta$ and $s_2(\theta) = (1+\cos\theta)\sin\theta$. Suppose that $s_{n-1}(\theta) > 0$ for some $n \geq 3$. Then the minimal values of $s_n(\theta)$ are certainly attained at some points $2\ell\pi/n \in (0, \pi)$, whose values are
$$
s_n\left(\frac{2\ell\pi}{n}\right) = s_{n-1}\left(\frac{2\ell\pi}{n}\right) + \frac{\sin 2\ell\pi}{n}
= s_{n-1}\left(\frac{2\ell\pi}{n}\right) > 0.
$$

Therefore $s_n(\theta) > 0$ on the interval $(0, \pi)$.
"
Sequences and Limits,"For any real number $\theta$ and any positive integer $n$, show the inequality
$$
\frac{\cos\theta}{2} + \frac{\cos 2\theta}{3} + \cdots + \frac{\cos n\theta}{n+1} \geq -\frac{1}{2}.
$$","The proof is substantially based on Verblunsky (1945). Write $\theta$ for $2\vartheta$ for brevity. Let $c_n(\vartheta)$ be the left-hand side of the inequality to be shown. It suffices to confine ourselves to the interval $[0, \pi/2]$. Clearly $c_1(\vartheta) = \cos\vartheta/2 \geq -1/2$ and
$$
c_2(\vartheta) = \frac{2}{3} \cos^2\vartheta + \frac{1}{2} \cos\vartheta - \frac{1}{3} \geq -\frac{41}{96},
$$
and we assume that $n \geq 3$. Note that
$$
\cos n\theta = \frac{\sin(2n+1)\vartheta - \sin(2n-1)\vartheta}{2\sin\vartheta}
= \frac{\sin^2(n+1)\vartheta - 2\sin^2 n\vartheta + \sin^2(n-1)\vartheta}{2\sin^2\vartheta},
$$
whose numerator is the second difference of the positive sequence $\{\sin^2 n\vartheta\}$. Using this formula we get
$$
c_n(\vartheta) = \frac{1}{2\sin^2\vartheta} \sum_{k=1}^n \frac{\sin^2(k+1)\vartheta - 2\sin^2k\vartheta + \sin^2(k-1)\vartheta}{k+1},
$$
which can be written as
$$
\frac{1}{2\sin^2\vartheta} \left(-\frac{2\sin^2\vartheta}{3} + \frac{\sin^2 2\vartheta}{12} + \cdots + \frac{2\sin^2(n-1)\vartheta}{n(n^2-1)}
-\frac{(n-1)\sin^2n\vartheta}{n(n+1)} + \frac{\sin^2(n+1)\vartheta}{n+1}\right).
$$

Hence we obtain
$$
c_n(\vartheta) \geq -\frac{1}{6} + \frac{\cos^2\vartheta}{6} + \frac{\sin^2(n+1)\vartheta - \sin^2n\vartheta}{2(n+1)\sin^2\vartheta}.
$$
For any $\vartheta$ satisfying $\sin(2n+1)\vartheta \geq 0$, we obviously have $c_n(\vartheta) \geq -1/3$. Moreover, if $\vartheta$ belongs to the interval $(3\pi/(2n+1), \pi/2)$, then using Jordan's inequality $\sin\vartheta \geq 2\vartheta/\pi$,
$$
c_n(\vartheta) \geq -\frac{1}{3} - \frac{1}{2(n+1)\sin(3\pi/(2n+1))}
\geq -\frac{1}{3} - \frac{2n+1}{12(n+1)} > -\frac{1}{2}.
$$
Thus it suffices to consider the interval $[\pi/(2n+1), 2\pi/(2n+1)]$.

In general, we consider an interval of the form
$$
\left[\frac{\alpha\pi}{2n+1}, \frac{\beta\pi}{2n+1}\right].
$$
For any $\vartheta$ satisfying $\sin(2n+1)\vartheta \leq c$ on this interval, it follows that
$$
c_n(\vartheta) \geq -\frac{1}{6} - \frac{\sin^2\vartheta}{6} - \frac{c}{2(n+1)\sin\vartheta}.
$$

Now the right-hand side can be written as $-1/6 - \varphi(\sin\vartheta)$, where $\varphi(x)$ is a concave function; hence, the maximum of $\varphi$ is attained at an endpoint of that interval. By using
$$
\alpha\pi\sin\vartheta \geq 7\vartheta\sin\frac{\alpha\pi}{7},
$$
we get
$$
\varphi\left(\sin\frac{\alpha\pi}{2n+1}\right) = \frac{1}{6} \sin^2\frac{\alpha\pi}{2n+1} + \frac{c}{2(n+1)\sin(\alpha\pi/(2n+1))}
$$
$$
\leq \frac{(\alpha\pi)^2}{6(2n+1)^2} + \frac{c}{2(n+1)} \cdot \frac{2n+1}{7\sin(\alpha\pi/7)}.
$$

Since $n \geq 3$, the last expression is less than
$$
\frac{(\alpha\pi)^2}{294} + \frac{c}{7\sin(\alpha\pi/7)}.
$$

Similarly, we get an estimate for another endpoint. For $\alpha=1$ and $\beta=4/3$, we can take $c=\sqrt{3}/2$ so that the value of $\varphi$ at the corresponding endpoint is less than 0.319 and 0.28 respectively. Similarly for $\alpha=4/3$ and $\beta=2$, we can take $c=1$ so that the value of $\varphi$ is less than 0.314 and 0.318 respectively. Therefore the maximum of $\varphi$ on the interval $[\pi/(2n+1), 2\pi/(2n+1)]$ is less than $1/3$, which implies that $c_n(\vartheta) > -1/2$.
"
Sequences and Limits,"Given a positive sequence $\{a_n\}_{n \geq 0}$ satisfying $\sqrt{a_1} \geq \sqrt{a_0} + 1$ and
$$
\left| a_{n+1} - \frac{a_n^2}{a_{n-1}} \right| \leq 1
$$
for any positive integer $n$, show that
$$
\frac{a_{n+1}}{a_n}
$$
converges as $n \to \infty$. Show moreover that $a_n \theta^{-n}$ converges as $n \to \infty$, where $\theta$ is the limit of the sequence $a_{n+1}/a_n$.","We first show that
\[
\frac{a_{n+1}}{a_n} > 1 + \frac{1}{\sqrt{a_0}}
\tag{1.1}
\]
by induction on $n$. When $n=0$ this holds by the assumption. Put $\alpha = 1 + 1/\sqrt{a_0}$ for brevity. Suppose that (1.1) holds for $n \leq m$. We then have $a_k > \alpha^k a_0$ for $1 \leq k \leq m+1$. Thus
\[
\left| \frac{a_{m+2}}{a_{m+1}} - \frac{a_1}{a_0} \right| 
\leq \sum_{k=1}^{m+1} \left| \frac{a_{k+1}}{a_k} - \frac{a_k}{a_{k-1}} \right|
\leq \sum_{k=1}^{m+1} \frac{1}{a_k},
\]
which is less than
\[
\frac{1}{a_0} \sum_{k=1}^{m+1} \alpha^{-k} < \frac{1}{a_0(\alpha - 1)} = \frac{1}{\sqrt{a_0}}.
\]

Therefore
\[
\frac{a_{m+2}}{a_{m+1}} > \frac{a_1}{a_0} - \frac{1}{\sqrt{a_0}} > 1 + \frac{1}{\sqrt{a_0}};
\]
thus (1.1) holds also for $n=m+1$.

Let $p > q$ be any positive integers. In the same way,
\[
\left| \frac{a_{p+1}}{a_p} - \frac{a_{q+1}}{a_q} \right|
\leq \sum_{k=q+1}^p \left| \frac{a_{k+1}}{a_k} - \frac{a_k}{a_{k-1}} \right|
\leq \sum_{k=q+1}^p \frac{1}{a_k},
\]
which is less than
\[
\frac{1}{a_q} \sum_{k=1}^{p-q} \frac{1}{\alpha^k} < \frac{\sqrt{a_0}}{a_q}.
\]

This means that the sequence $\{a_{n+1}/a_n\}$ satisfies the Cauchy criterion since $a_q$ diverges to $\infty$ as $q \to \infty$. Letting $p \to \infty$ in the above inequalities, we get
\[
\left| \frac{a_{q+1}}{a_q} - \theta \right| \leq \frac{\sqrt{a_0}}{a_q}.
\]

Multiplying both sides by $a_q / \theta^{q+1}$, we have
\[
\left| \frac{a_{q+1}}{\theta^{q+1}} - \frac{a_q}{\theta^q} \right| 
\leq \frac{\sqrt{a_0}}{\theta^{q+1}},
\]
which shows that the sequence $\{a_n / \theta^n\}$ also satisfies the Cauchy criterion.
"
Sequences and Limits,"Let $E$ be any bounded closed set in the complex plane containing an infinite number of points, and let $M_n$ be the maximum of $|V(x_1, \dots, x_n)|$ as the points $x_1, \dots, x_n$ run through the set $E$, where
$$
V(x_1, \dots, x_n) = \prod_{1 \leq i < j \leq n} (x_i - x_j)
$$
is the Vandermonde determinant. Show that $M_n^{2/(n(n-1))}$ converges as $n \to \infty$.","Let $\xi_1, \dots, \xi_{n+1}$ be the points at which $|V(x_1, \dots, x_{n+1})|$ attains its maximum $M_{n+1}$. Since
\[
\frac{V(\xi_1, \dots, \xi_{n+1})}{V(\xi_1, \dots, \xi_n)} = (\xi_1 - \xi_{n+1}) \cdots (\xi_n - \xi_{n+1}),
\]
we have
\[
\frac{M_{n+1}}{M_n} \leq |\xi_1 - \xi_{n+1}| \cdots |\xi_n - \xi_{n+1}|.
\]

Applying the same argument to each point $\xi_1, \dots, \xi_n$, we get $n+1$ similar inequalities whose product gives
\[
\left( \frac{M_{n+1}}{M_n} \right)^{n+1} \leq \prod_{i \neq j} |\xi_i - \xi_j| = M_n^2.
\]

Hence the sequence $M_n^{2/(n(n-1))}$ is monotone decreasing."
Infinite Series,"As is well-known, the harmonic series
\[
\frac{1}{1} + \frac{1}{2} + \frac{1}{3} + \cdots + \frac{1}{n} + \cdots
\]
diverges to $\infty$. Show, however, that the convergence of the subseries removing all terms containing the digit ""7"" in the decimal expression of the denominator.","Any integer in the interval $[10^{k-1}, 10^k)$ has $k$ digits in its decimal expansion. Among these there exist exactly $8 \cdot 9^{k-1}$ integers which do not contain the digit `7' in their decimal expansions. Thus the sum of the subseries defined in the problem, say $S$, can be estimated from above as
\[
S < \sum_{k=1}^\infty \frac{8 \cdot 9^{k-1}}{10^{k-1}} = 80.
\]
"
Infinite Series,"Given two series $\sum_{n=0}^\infty a_n$ and $\sum_{n=0}^\infty b_n$,
\[
\sum_{n=0}^\infty (a_0b_n + a_1b_{n-1} + \cdots + a_nb_0)
\]
is called the Cauchy product of $\sum a_n$ and $\sum b_n$.
Suppose that $\sum a_n$ and $\sum b_n$ converge to $\alpha$ and $\beta$ respectively and that the Cauchy product converges to $\delta$. Show then that $\alpha \beta$ is equal to $\delta$.","Since $a_n \to 0$ and $b_n \to 0$ as $n \to \infty$, both power series 
\[
f(x) = \sum_{n=0}^\infty a_n x^n \quad \text{and} \quad g(x) = \sum_{n=0}^\infty b_n x^n
\]
converge absolutely for $|x| < 1$. Hence the product
\[
f(x)g(x) = \sum_{n=0}^\infty (a_0b_n + a_1b_{n-1} + \cdots + a_nb_0) x^n
\]
also converges for $|x| < 1$. It follows from Abel's continuity theorem that $f(x)$, $g(x)$, and $f(x)g(x)$ converge to $\alpha$, $\beta$, and $\alpha \beta$ as $x \to 1^-$. Thus $\delta = \alpha \beta$.
"
Infinite Series,"Given two series $\sum_{n=0}^\infty a_n$ and $\sum_{n=0}^\infty b_n$,
\[
\sum_{n=0}^\infty (a_0b_n + a_1b_{n-1} + \cdots + a_nb_0)
\]
is called the Cauchy product of $\sum a_n$ and $\sum b_n$.
Suppose that $\sum a_n$ converges absolutely to $\alpha$ and that $\sum b_n$ converges to $\beta$. Show that the Cauchy product converges to $\alpha \beta$.","Let 
\[
M = \sum_{n=0}^\infty |a_n| \quad \text{and} \quad s_n = b_0 + b_1 + \cdots + b_n
\]
with $|s_n| \leq K$ for some constant $K > 0$. By (a), it suffices to show the convergence of the Cauchy product. To see this, put
\[
c_n = \sum_{k=0}^n \left(a_0 b_k + a_1 b_{k-1} + \cdots + a_k b_0\right)
= a_0 s_n + a_1 s_{n-1} + \cdots + a_n s_0.
\]

For any $\epsilon > 0$, there exists an integer $N$ satisfying $|s_p - s_q| < \epsilon$ and 
\[
|a_q| + \cdots + |a_p| < \epsilon
\]
for any integers $p$ and $q$ with $p > q \geq N$. Then for all $p > q > 2N$, we get
\[
|c_p - c_q| = \left| \sum_{k=0}^N a_k (s_{p-k} - s_{q-k}) + \sum_{k=N+1}^q a_k (s_{p-k} - s_{q-k}) + \sum_{k=q+1}^p a_k s_{p-k} \right|.
\]

Clearly, the first sum on the right-hand side is estimated above by
\[
M \max_{0 \leq k \leq N} |s_{p-k} - s_{q-k}| < M \epsilon.
\]

The second and third sums are similarly estimated above by
\[
2K \sum_{k=N+1}^p |a_k| < 2K \epsilon.
\]

This implies that the sequence $\{c_n\}$ satisfies the Cauchy criterion."
Infinite Series,"Given two series $\sum_{n=0}^\infty a_n$ and $\sum_{n=0}^\infty b_n$,
\[
\sum_{n=0}^\infty (a_0b_n + a_1b_{n-1} + \cdots + a_nb_0)
\]
is called the Cauchy product of $\sum a_n$ and $\sum b_n$.
Give an example of two convergent series whose Cauchy product is divergent.","For example, take
\[
a_n = b_n = \frac{(-1)^n}{\sqrt{n+1}}.
\]
Obviously $\sum a_n$ and $\sum b_n$ converge. However, we have
\[
\left|a_0 b_n + a_1 b_{n-1} + \cdots + a_n b_0\right| = \sum_{k=1}^{n+1} \frac{1}{\sqrt{k(n+2-k)}}
\geq \sum_{k=1}^{n+1} \frac{2}{k+n+2-k},
\]
which shows the divergence of the Cauchy product, since the last expression is greater than 1.
"
Infinite Series,"For any positive sequence $\{a_n\}_{n \geq 1}$ show the inequality
\[
\sum_{n=1}^\infty (a_1 a_2 \cdots a_n)^{1/n} < e \sum_{n=1}^\infty a_n.
\]

Prove further that the constant $e$ on the right-hand side cannot in general be replaced by any smaller number.","The proof is based on Pólya (1926). Let $\{b_n\}$ be an arbitrary positive sequence. First we write
\[
\sum_{n=1}^m (a_1 a_2 \cdots a_n)^{1/n} = \sum_{n=1}^m \left( \frac{a_1 b_1 a_2 b_2 \cdots a_n b_n}{b_1 b_2 \cdots b_n} \right)^{1/n}.
\]

Using the arithmetic-geometric mean inequality on the right-hand side, the above sum is less than or equal to
\[
\sum_{n=1}^m \frac{1}{(b_1 b_2 \cdots b_n)^{1/n}} \cdot \frac{a_1 b_1 + a_2 b_2 + \cdots + a_n b_n}{n}
= \sum_{k=1}^m a_k b_k \sum_{n=k}^m \frac{1}{n (b_1 b_2 \cdots b_n)^{1/n}}.
\]

We now take 
\[
b_n = n \left(1 + \frac{1}{n}\right)^n
\]
so that $(b_1 b_2 \cdots b_n)^{1/n} = n + 1$. Therefore we have
\[
\sum_{k=1}^m a_k b_k \sum_{n=k}^m \frac{1}{n (n+1)} = \sum_{k=1}^m a_k b_k \left( \frac{1}{k} - \frac{1}{m+1} \right)
< \sum_{k=1}^m a_k \left( 1 + \frac{1}{k} \right)^k
\]
and this is smaller than $e \sum_{k=1}^m a_k$. Note that the equality does not occur in the original inequality when $m \to \infty$.

To see that $e$ cannot be replaced by any smaller number, we take, for example,
\[
a_n = 
\begin{cases} 
n^{-1} & \text{for } 1 \leq n \leq m, \\
2^{-n} & \text{for } n > m,
\end{cases}
\]
where $m$ is an integer parameter. Then it is not hard to see that
\[
\sum_{n=1}^\infty (a_1 a_2 \cdots a_n)^{1/n} = \sum_{n=1}^m n^{1-1/n} + O(1)
= e \log m + O(1),
\]
and
\[
\sum_{n=1}^\infty a_n = 1 + \sum_{n=1}^m \frac{1}{n} = \log m + O(1),
\]
which implies that the ratio of the above two sums converges to $e$ as $m \to \infty$."
Infinite Series,"For any positive sequence $\{a_n\}_{n \geq 1}$ show the inequality
\[
\left( \sum_{n=1}^\infty a_n \right)^4 < \pi^2 \left( \sum_{n=1}^\infty a_n^2 \right) \left( \sum_{n=1}^\infty n^2 a^2_n \right).
\]

Prove further that the constant $\pi^2$ on the right-hand side cannot in general be replaced by any smaller number.","Put 
\[
\alpha = \sum_{n=1}^\infty a_n^2 \quad \text{and} \quad \beta = \sum_{n=1}^\infty n^2 a_n^2
\]
for brevity. We of course assume that $\beta$ is finite. Introducing two positive parameters $\sigma$ and $\tau$, we first write
\[
\left(\sum_{n=1}^\infty a_n\right)^2 = \left(\sum_{n=1}^\infty a_n \sqrt{\sigma + \tau n^2} \cdot \frac{1}{\sqrt{\sigma + \tau n^2}} \right)^2.
\]
Using the Cauchy-Schwarz inequality, this is less than or equal to
\[
\sum_{n=1}^\infty \frac{1}{\sigma + \tau n^2} \sum_{n=1}^\infty a_n^2 (\sigma + \tau n^2) = (\sigma \alpha + \beta \tau) \sum_{n=1}^\infty \frac{1}{\sigma + \tau n^2}.
\]
Since the function $1/(\sigma + \tau x^2)$ is monotone decreasing on $[0, \infty)$, we obtain
\[
\sum_{n=1}^\infty \frac{1}{\sigma + \tau n^2} < \int_0^\infty \frac{dx}{\sigma + \tau x^2} = \frac{\pi}{2 \sqrt{\sigma \tau}},
\]
which implies that
\[
\left(\sum_{n=1}^\infty a_n\right)^2 < \frac{\pi}{2} \cdot \frac{\sigma \alpha + \beta \tau}{\sqrt{\sigma \tau}}.
\]
The right-hand side attains its minimum $\pi \sqrt{\alpha \beta}$ at $\sigma / \tau = \beta / \alpha$.

To see that $\pi^2$ cannot be replaced by any smaller number, we take, for example,
\[
a_n = \frac{\sqrt{\rho}}{\rho + n^2}
\]
with a positive parameter $\rho$. It then follows from
\[
\sqrt{\rho} \int_1^\infty \frac{dx}{\rho + x^2} < \sum_{n=1}^\infty a_n < \sqrt{\rho} \int_0^\infty \frac{dx}{\rho + x^2}
\]
that
\[
\sum_{n=1}^\infty a_n = \frac{\pi}{2} + O\left(\frac{1}{\sqrt{\rho}}\right)
\]
as $\rho \to \infty$. Similarly, we have
\[
\sum_{n=1}^\infty a_n^2 = \frac{\pi}{4 \sqrt{\rho}} + O\left(\frac{1}{\rho}\right)
\]
and
\[
\sum_{n=1}^\infty n^2 a_n^2 = \sqrt{\rho} \sum_{n=1}^\infty a_n - \rho \sum_{n=1}^\infty a_n^2 = \frac{\pi}{4} \sqrt{\rho} + O(1),
\]
which imply that
\[
\left(\sum_{n=1}^\infty a_n\right)^4 = \frac{\pi^4}{16} + O\left(\frac{1}{\sqrt{\rho}}\right),
\]
and
\[
\sum_{n=1}^\infty a_n^2 \sum_{n=1}^\infty n^2 a_n^2 = \frac{\pi^2}{16} + O\left(\frac{1}{\sqrt{\rho}}\right)
\]
as $\rho \to \infty$."
Infinite Series,Show that the convergence of $\sum_{n=1}^\infty n a_n$ implies that of $\sum_{n=1}^\infty a_n$.,"Put 
\[
b_n = a_1 + 2a_2 + \cdots + n a_n,
\]
and let $M$ be the least upper bound of $|b_n|$. For any $\epsilon > 0$ and any integers $p, q$ with $p > q > 2M / \epsilon$, we have
\[
\left| \sum_{n=q}^p a_n \right| = \left| \frac{b_q - b_{q-1}}{q} + \frac{b_{q+1} - b_q}{q+1} + \cdots + \frac{b_p - b_{p-1}}{p} \right|.
\]

This can be written as
\[
\left| \sum_{n=q}^p a_n \right| = \left| \frac{-b_{q-1}}{q} + \left( \frac{1}{q} - \frac{1}{q+1} \right)b_q + \cdots + \left( \frac{1}{p-1} - \frac{1}{p} \right)b_{p-1} + \frac{b_p}{p} \right|.
\]

This is bounded above as follows:
\[
\left| \sum_{n=q}^p a_n \right| \leq \frac{2M}{q},
\]
which is clearly less than $\epsilon$. This is nothing but the Cauchy criterion for the convergence of the series $\sum a_n$.
\qed"
Infinite Series,Show that $\frac{1}{n} \sum_{k=1}^n a_k$ converges to $0$ when $\sum_{n=1}^\infty \frac{a_n}{n}$ converges.,"Put
\[
\frac{a_1}{1} + \frac{a_2}{2} + \cdots + \frac{a_n}{n} = \alpha + \epsilon_n \quad \text{and} \quad
\sigma_n = \frac{a_1 + a_2 + \cdots + a_n}{n},
\]
where $\epsilon_n$ is a sequence converging to $0$.

We now show that
\[
\sigma_n = \frac{\alpha}{n} - \frac{\epsilon_1 + \epsilon_2 + \cdots + \epsilon_{n-1}}{n} + \epsilon_n \tag{2.3}
\]
by induction on $n$. The case $n=1$ is clear, since $\sigma_1 = a_1 = \alpha + \epsilon_1$. Suppose next that $(2.3)$ holds for $n=m$. We then have
\[
\sigma_{m+1} = \frac{m}{m+1} \sigma_m + \frac{a_{m+1}}{m+1}
\]
\[
= \frac{m}{m+1} \left( \frac{\alpha}{m} - \frac{\epsilon_1 + \cdots + \epsilon_{m-1}}{m} + \epsilon_m \right) + \frac{a_{m+1}}{m+1}
\]
\[
= \frac{\alpha}{m+1} - \frac{\epsilon_1 + \cdots + \epsilon_m}{m+1} + \epsilon_{m+1},
\]
as required. Obviously $(2.3)$ implies that $\sigma_n$ converges to $0$ as $n \to \infty$. \qed"
Infinite Series,"Suppose that $\{a_n\}_{n \geq 1}$ is a non-negative sequence satisfying $\sum_{n=1}^\infty a_n = \infty$.
Then show that the series
\[
\sum_{n=1}^\infty \frac{a_n}{\left(a_1 + a_2 + \cdots + a_n\right)^\alpha}
\]
converges when $\alpha > 1$, and diverges when $0 < \alpha \leq 1$.","Put $s_n = a_1 + a_2 + \cdots + a_n$. The convergence is clear when $\alpha > 1$, since
\[
\sum_{n=2}^\infty \frac{a_n}{s_n^\alpha} \leq \sum_{n=2}^\infty \int_{s_{n-1}}^{s_n} \frac{dx}{x^\alpha} = \int_{s_1}^\infty \frac{dx}{x^\alpha} < \infty.
\]

When $0 < \alpha \leq 1$, it suffices to consider the case $\alpha = 1$ since $s_n > 1$ for all sufficiently large $n$. Suppose first that $s_{n-1} < a_n$ for infinitely many $n$'s. We then have $a_n / s_n > 1/2$ for infinitely many $n$'s, which shows the divergence of the series. 

Consider next the case in which $s_{n-1} \geq a_n$ for all integers $n$ greater than some integer $N$. Then
\[
\sum_{n>N} \frac{a_n}{s_n} \geq \frac{1}{2} \sum_{n>N} \int_{s_{n-1}}^{s_n} \frac{dx}{x}
= \frac{1}{2} \int_{s_N}^\infty \frac{dx}{x} = \infty.
\]

Note that the divergence of the sequence $s_n$ is essential in the second case. \qed"
Infinite Series,"Suppose that $\sum_{n=1}^\infty a_n b_n$ converges for any sequence $\{b_n\}$ converging. 
Then show that $\sum_{n=1}^\infty a_n$ converges absolutely.","Without loss of generality, we can assume that $\{a_n\}$ is a non-negative sequence, since we can take $-b_n$ instead of $b_n$. Suppose, on the contrary, that the series $\sum a_n$ diverges to $\infty$. It follows from the result of \textbf{Problem 2.7} that
\[
\sum_{n=1}^\infty \frac{a_n}{a_1 + a_2 + \cdots + a_n} = \infty,
\]
contrary to the assumption since
\[
b_n = \frac{1}{a_1 + a_2 + \cdots + a_n}
\]
converges to $0$ as $n \to \infty$. \qed"
Infinite Series,"Suppose that $\sum_{n=1}^\infty a_n b_n$ converges for any sequence $\{b_n\}$ such that $\sum_{n=1}^\infty b_n^2$ converges. 
Then show that $\sum_{n=1}^\infty a_n^2$ also converges.","Suppose, on the contrary, that $\sum a_n^2$ diverges to $\infty$. By the result of \textbf{Problem 2.7}, we have
\[
\sum_{n=1}^\infty \frac{a_n^2}{(a_1^2 + a_2^2 + \cdots + a_n^2)^2} < \infty,
\]
contrary to the assumption since
\[
b_n = \frac{a_n}{a_1^2 + a_2^2 + \cdots + a_n^2}
\]
satisfies
\[
\sum_{n=1}^\infty a_n b_n = \sum_{n=1}^\infty \frac{a_n^2}{a_1^2 + a_2^2 + \cdots + a_n^2} = \infty. \qed"
Infinite Series,"The limit
\[
\gamma = \lim_{n \to \infty} \left( 1 + \frac{1}{2} + \cdots + \frac{1}{n} - \log n \right) 
= 0.5772156649015328606065120\ldots
\]
is called Euler's constant or sometimes the Euler-Mascheroni constant. Show that the following series converges to $\gamma$:
\[
\frac{1}{2} - \frac{1}{3} + 2 \left( \frac{1}{4} - \frac{1}{5} + \frac{1}{6} - \frac{1}{7} \right) 
+ 3 \left( \frac{1}{8} - \frac{1}{9} + \cdots - \frac{1}{15} \right) + \cdots.
\]","For brevity put
\[
\sigma_n = 1 - \frac{1}{2} + \frac{1}{3} - \cdots + \frac{1}{2n - 1} - \log 2
\]
for any positive integer $n$. It is easily seen that
\[
\sigma_1 + \sigma_2 + \cdots + \sigma_n = 1 + \frac{1}{2} + \cdots + \frac{1}{2n - 1} - n \log 2;
\]
therefore
\[
\gamma = \lim_{n \to \infty} (\sigma_1 + \sigma_2 + \cdots + \sigma_n).
\]
We have Vacca's formula by noticing that $\sigma_n = \tau_n + \tau_{n+1} + \cdots$, where
\[
\tau_n = \frac{1}{2^n} - \frac{1}{2^n + 1} + \frac{1}{2^n + 2} - \cdots - \frac{1}{2^{n+1} - 1}.
\]"
Infinite Series,"Making use of the formula
\[
\frac{\sin(2n+1)\theta}{(2n+1)\sin \theta} = \prod_{k=1}^n \left(1 - \frac{\sin^2 \theta}{\sin^2 k\pi / (2n+1)} \right),
\]
show that
\[
\frac{\sin \pi x}{\pi x} = \prod_{n=1}^\infty \left( 1 - \frac{x^2}{n^2} \right)
\]
holds for all real $x$.","Since it is easily verified that $\sin(2n+1)\theta$ is a polynomial of $\sin \theta$ by induction, we can write
\[
p_n(\sin^2 \theta) = \frac{\sin(2n+1)\theta}{\sin \theta}
\]
where $p_n(x)$ is a polynomial of degree $n$ satisfying $p_n(0) = 2n+1$. The zeros of $p_n(x)$ can be obtained by solving the equation $\sin(2n+1)\theta = 0$ with $\sin \theta \neq 0$; therefore we get the following $n$ points:
\[
\sin^2 \xi_{1,n} < \sin^2 \xi_{2,n} < \cdots < \sin^2 \xi_{n,n}
\]
in the interval $(0,1)$ where $\xi_{k,n} = k\pi/(2n+1) \in (0, \pi/2)$. Hence we have
\[
\sin(2n+1)\theta = (2n+1) \sin \theta \prod_{k=1}^n \left(1 - \frac{\sin^2 \theta}{\sin^2 \xi_{k,n}}\right).
\]

By the substitution $x = (2n+1)\theta/\pi$,
\[
\frac{\sin \pi x}{\pi x} \cdot \frac{x_n}{\sin x_n} = \prod_{k=1}^n \left(1 - \frac{\sin^2 x_n}{\sin^2 \xi_{k,n}}\right),
\]
where $x_n = \pi x / (2n+1)$.

We can assume $x$ is not an integer; otherwise, the expansion clearly holds. Take any positive integers $n$ and $m$ satisfying $n > m > |x|$ so that $|x_n| < \xi_{k,n}$. Putting
\[
\eta_{m,n} = \prod_{k=m+1}^n \left(1 - \frac{\sin^2 x_n}{\sin^2 \xi_{k,n}}\right),
\]
we get
\[
\lim_{n \to \infty} \frac{1}{\eta_{m,n}} = \frac{\pi x}{\sin \pi x} \prod_{k=1}^m \left(1 - \frac{x^2}{k^2}\right).
\]

On the other hand,
\[
1 > \eta_{m,n} \geq 1 - \sum_{k=m+1}^n \frac{\sin^2 x_n}{\sin^2 \xi_{k,n}}
\]
since
\[
(1 - \alpha_1)(1 - \alpha_2) \cdots (1 - \alpha_n) \geq 1 - \alpha_1 - \cdots - \alpha_n
\]
for any $0 < \alpha_k < 1$ and any positive integer $N$. Using now the inequalities
\[
\frac{2\theta}{\pi} < \sin \theta < \theta
\]
holding for $0 < \theta < \pi/2$,
\[
\eta_{m,n} \geq 1 - \frac{\pi^2}{4} \sum_{k=m+1}^n \frac{x_n^2}{\xi_{k,n}^2} \geq 1 - \frac{\pi^2 x^2}{4m},
\]
since
\[
\sum_{k=m+1}^\infty \frac{1}{k^2} < \frac{1}{m}.
\]

Hence $\lim_{n \to \infty} \eta_{m,n}$ converges to $1$ as $m \to \infty$."
Continuous Functions,"Suppose that $f \in C(\mathbb{R})$ and that $f(x+1) - f(x)$ converges to $0$ as $x \to \infty$. Then show that
\[
\frac{f(x)}{x}
\]
also converges to $0$ as $x \to \infty$. Suppose further that $f(x+y) - f(x)$ converges to $0$ as $x \to \infty$ for an arbitrary fixed $y$. Show then that this convergence is uniform on compact sets in $\mathbb{R}$.","For any $\epsilon > 0$ there exists an integer $N$ satisfying
\[
-\epsilon < f(x+1) - f(x) < \epsilon
\]
for any $x > N$. Summing the following $\ell = \lfloor x \rfloor - N$ inequalities
\[
-\epsilon < f(x-j+1) - f(x-j) < \epsilon
\]
for $j = 1, \ldots, \ell$ and for $x \geq N+1$ we get
\[
-\epsilon(\lfloor x \rfloor - N) < f(x) - f(x-\ell) < \epsilon(\lfloor x \rfloor - N).
\]
Since $N \leq x - \ell < N+1$, it follows from this that
\[
-M - \epsilon(x - N + 1) < f(x) < M + \epsilon(x - N),
\]
where $M$ is the maximum of $|f(x)|$ on $[N, N+1]$. Therefore
\[
\frac{|f(x)|}{x} < \frac{M + \epsilon(x - N + 1)}{x} < \epsilon + \frac{M}{x},
\]
which implies that $|f(x)/x| < 2\epsilon$ for any $x > \max\{N+1, M/\epsilon\}$.

To show the latter half of the problem put
\[
g_x(y) = \sup_{t \geq x} |f(t+y) - f(t)|,
\]
which converges to $0$ as $x \to \infty$ for an arbitrary fixed $y$. If $g_x(y_0) > s$, then there exists $t_0 \geq x$ satisfying $|f(t_0+y_0) - f(t_0)| > s$. By the continuity of $f$, we have $|f(t+y) - f(t)| > s$ for any $(t, y)$ sufficiently close to $(t_0, y_0)$. This means that $g_x(y) > s$ for any $y$ sufficiently close to $y_0$; in other words, the set
\[
\{y \in \mathbb{R} : g_x(y) > s\}
\]
is an open set. Hence $\{g_n(y)\}$ is a sequence of Borel measurable functions converging pointwise to $0$. By Egoroff's theorem we can find a measurable set $F \subset [-1, 1]$ whose measure is greater than $3/2$ such that $g_n(y)$ converges to $0$ uniformly on $F$; therefore for any $\epsilon > 0$ there exists an integer $N$ satisfying $g_n(y) < \epsilon$ for any $n > N$ and any $y \in F$. By the theorem due to Steinhaus (1920) there exists an interval $I$ such that any point $y$ in $I$ can be expressed as $y = u - v$ with $u, v \in F \cap (-F)$, since the measure of $F \cap (-F)$ is positive. Clearly $-y$ has such an expression, and we can assume that $I$ is contained in the positive real axis. For any non-negative $x$ we have
\[
g_n(x+x') = \sup_{t \geq n} |f(t+x+x') - f(t)| \leq \sup_{t \geq n} |f(t+x) - f(t)| + \sup_{t \geq n} |f(t+x') - f(t+x)| = g_n(x) + g_n(x').
\]
Applying the above inequality to $y = u - v \in I$ we get
\[
g_n(y) = g_n(u-v) \leq g_n(u) + g_n(-v) < 2\epsilon,
\]
since the case does not occur in which both $u$ and $-v$ are negative. This implies that $\{g_n(y)\}$ converges to $0$ uniformly on the interval $I$. This is also true on any set consisting of finite parallel translations of $I$ since $g_n(x+x') \leq g_n(x) + g_n(x')$ for any $x \in I$ and $x'$."
Continuous Functions,"Let $p_n(x)$ be any polynomial with integer coefficients whose degree is greater than or equal to $1$, and let $I$ be any closed interval of length $\geq 4$. Then show that there exists at least one point $x$ in $I$ satisfying
\[
|p_n(x)| \geq 2.
\]","Let $I$ be any closed interval of length 4. We solve this problem for any polynomial 
\[
p_n(x) = a_n x^n + a_{n-1}x^{n-1} + \cdots + a_0
\]
with a non-zero integer $a_n$ and real $a_{n-1}, \ldots, a_0$. Since $a_n$ is invariant under parallel translation, we can assume that $I = [-2, 2]$. Let $M$ be the difference of the maximum and the minimum of $p_n$ on the interval $I$. It may be convenient to introduce the notation
\[
\sum_{k=0}^n{}^* b_k = b_0 + 2b_1 + 2b_2 + \cdots + 2b_{n-1} + b_n.
\]

Now for any integer $0 \leq s < n$, put $\omega = -e^{\pi i / n} \neq 1$; hence
\[
\sum_{k=0}^n{}^* \omega^k = \frac{(1+\omega)(1-\omega^n)}{1-\omega} = (1 - (-1)^{s+n}) \frac{1+\omega}{1-\omega}.
\]

Since $\overline{\omega} = \omega^{-1}$, it can be seen that the real part of the above expression vanishes; in other words,
\[
\sum_{k=0}^n{}^* (-1)^k \cos\frac{ks}{n}\pi = 0.
\]

On the other hand, it is clear that $2\cos s\theta = e^{i s \theta} + e^{-i s \theta}$ is a polynomial in $2\cos\theta = e^{i\theta} + e^{-i\theta}$ with integer coefficients of degree $s$. Hence we can write
\[
2\cos s\theta = \tau_s(2\cos\theta).
\]

Since $s$ is arbitrary, we get
\[
\sum_{k=0}^n{}^* (-1)^k \alpha_k^s = 0
\]
for any $0 \leq s < n$, where 
\[
\alpha_k = 2\cos\frac{k\pi}{n}.
\]

Also for $s = n$,
\[
\sum_{k=0}^n{}^* (-1)^k \alpha_k^n = \sum_{k=0}^n{}^* (-1)^k \tau_n(\alpha_k) = 2\sum_{k=0}^n{}^* (-1)^k \cos k\pi = 4n,
\]
since the coefficient of the leading term of $\tau_n(x)$ is equal to 1. Hence we have
\[
4n|a_n| = \left| \sum_{k=0}^n{}^* (-1)^k p_n(\alpha_k) \right| \leq \sum_{k=0}^{n-1} |p_n(\alpha_k) - p_n(\alpha_{k+1})| \leq nM,
\]
which implies that $M \geq 4|a_n| \geq 4$. We thus have $\max_{x \in I} |p_n(x)| \geq 2$. 
"
Continuous Functions,"Let $f_n \in C[a, b]$ be a monotone increasing sequence
\[
f_1(x) \leq f_2(x) \leq \cdots,
\]
which converges pointwise to $f(x) \in C[a, b]$. Show that the convergence is uniform on $[a, b]$.","For any $\epsilon > 0$ define the set
\[
E_n = \{x \in [a, b] : |f(x) - f_n(x)| \geq \epsilon\}.
\]

Then $\{E_n\}$ is a sequence of monotone decreasing compact sets in view of the continuity of $f_n$ and $f$. Suppose now that $E_n$ is not empty for any positive integer $n$. It then follows that
\[
\bigcap_{n=1}^\infty E_n \neq \emptyset.
\]

Let $x_0$ be some point belonging to all the sets $E_n$. But this means that $\{f_n(x_0)\}$ does not converge to $f(x_0)$, contrary to the assumption. Thus $E_n$ is empty for all sufficiently large $n$; in other words, $|f(x) - f_n(x)| < \epsilon$ for any $x \in [a, b]$. "
Continuous Functions,"Suppose that $f \in C[0, \infty)$ and that $f(nx)$ converges to $0$ as $n \to \infty$ for an arbitrary non-negative $x$. Prove or disprove that $f(x)$ converges to $0$ as $x \to \infty$.","We prove that the assertion is true. Suppose, on the contrary, that $f(x)$ does not converge to 0 as $x \to \infty$. We then find a strictly monotone increasing sequence $1 < x_1 < x_2 < \cdots$ diverging to $\infty$ and a positive constant $\delta$ satisfying
\[
|f(x_k)| > 2\delta
\]
for any positive integer $k$. By the continuity of $f$ we can find a sufficiently small $\epsilon_k > 0$ such that $|f(x)| \geq \delta$ holds on the interval $[x_k - \epsilon_k, x_k + \epsilon_k]$ for each $k$. Now put
\[
E_n = \bigcup_{k=n}^\infty \bigcup_{m=-\infty}^\infty \left( \frac{m - \epsilon_k}{x_k}, \frac{m + \epsilon_k}{x_k} \right)
\]
for all positive integers $n$. $E_n$ is an open and dense set since $x_k$ diverges to $\infty$ as $k \to \infty$. Since $\mathbb{R}$ is a Baire space, the intersection
\[
\bigcap_{n=1}^\infty E_n
\]
is also a dense set; thus we can choose a point $x^* > 1$ which belongs to all the sets $E_n$. Namely, there exist two integers $k_n \geq n$ and $m_n$ satisfying
\[
\left| x^* - \frac{m_n}{x_{k_n}} \right| < \frac{\epsilon_{k_n}}{x_{k_n}}
\]
for all $n$. Note that $m_n$ diverges to $\infty$ as $n \to \infty$. Therefore
\[
\left| x_{k_n} - \frac{m_n}{x^*} \right| < \frac{\epsilon_{k_n}}{x^*} < \epsilon_{k_n},
\]
which implies that $|f(m_n / x^*)| \geq \delta$, contrary to the assumption that $f(nx)$ converges to 0 as $n \to \infty$ at $x = 1 / x^*$."
Continuous Functions,"Show that there are no continuous functions $f$, $g$, and $h$ defined on $\mathbb{R}$ satisfying
\[
h(f(x) + g(y)) = xy
\]
for all points $(x, y)$ in $\mathbb{R}^2$.","If continuous functions $f, g$ and $h$ on $\mathbb{R}$ satisfy the relation
\[
h(f(x) + g(y)) = xy
\]
for all $x$ and $y$, the function $h(x)$ must be surjective onto $\mathbb{R}$ since $xy$ takes all real values. Now if $f(x) = f(x')$, then
\[
x = h(f(x) + g(1)) = h(f(x') + g(1)) = x'.
\]
Thus $f(x)$ is one-to-one and hence strictly monotone. Suppose that $f$ is bounded above. The limit of $f(x)$ as $x \to \infty$ exists, say $a$. We then see that
\[
h(a + g(1)) = \lim_{x \to \infty} h(f(x) + g(1)) = \lim_{x \to \infty} x = \infty,
\]
a contradiction. Thus $f(x)$ is unbounded above. The similar argument can be applied when $x \to -\infty$. Therefore $f$ is one-to-one onto $\mathbb{R}$. Since
\[
h(f(x) + g(0)) = 0
\]
holds for all $x$, the function $h(x)$ vanishes identically. This is clearly a contradiction."
Continuous Functions,"First put $E_1 = \{0, 1\}$ and suppose that a finite sequence $E_n \subset [0, 1]$ is given. 
Define $E_{n+1}$ by inserting new fractions $(a+c)/(b+d)$ between every two 
consecutive fractions $a/b$ and $c/d$ in $E_n$. Of course we understand $0 = 0/1$ 
and $1 = 1/1$; thus,
\[
E_2 = \left\{0, \frac{1}{2}, 1\right\}, \quad E_3 = \left\{0, \frac{1}{3}, \frac{1}{2}, \frac{2}{3}, 1\right\}, \dots
\]

With the help of the sequence $E_n$, we define the piecewise linear continuous function 
$\varphi_n(x)$ on the interval $[0, 1]$ such that
\[
\varphi_n\left(\frac{a}{b}\right) = \varphi_n\left(\frac{c}{d}\right) = 0, \quad \varphi_n\left(\frac{a+c}{b+d}\right) = \frac{1}{b+d},
\]
and $\varphi_n(x)$ is linear on the intervals
\[
\left[\frac{a}{b}, \frac{a+c}{b+d}\right] \quad \text{and} \quad \left[\frac{a+c}{b+d}, \frac{c}{d}\right],
\]
respectively, for every two successive terms $a/b$ and $c/d$ in $E_n$.

Show then that the series
\[
f(x) = \sum_{n=1}^\infty \varphi_n(x)
\]
converges at every point $x \in [0, 1]$; more precisely, it converges to $1 - 1/q$ at any rational 
point $x = p/q \in [0, 1]$ with $(p, q) = 1$ and to $1$ at any irrational point $x$ in $(0, 1)$.","Let $p/q$ be any fraction in the interval $[0, 1]$. The fraction $p/q$ is contained in $E_q$; otherwise, there exist two adjacent fractions $a/b$ and $c/d$ in $E_q$ satisfying
\[
\frac{a}{b} < \frac{p}{q} < \frac{c}{d},
\]
which implies
\[
\frac{1}{bd} = \frac{c}{d} - \frac{a}{b} = \frac{c}{d} - \frac{p}{q} + \frac{p}{q} - \frac{a}{b} \geq \frac{1}{dq} + \frac{1}{bq} \geq \frac{b+d}{bdq} > \frac{1}{bd},
\]
a contradiction. We used here the facts that $bc - ad = 1$ and $b + d \geq n + 1$ for any two consecutive fractions $a/b$ and $c/d$ in $E_n$, which can be easily verified by induction on $n$.

We next show that 
\[
f\left(\frac{a}{b}\right) = 1 - \frac{1}{b} \tag{3.1}
\]
for any fraction $a/b$ belonging to $E_n$ by induction on $n$. Clearly, it holds for $a/b = 0/1$ and $1/1$ since $\phi_m(0) = \phi_m(1) = 0$ for all positive integer $m$. Suppose (3.1) holds for any $a/b \in E_n$. By definition, we have
\[
f\left(\frac{a}{b}\right) = \phi_1\left(\frac{a}{b}\right) + \cdots + \phi_{b-1}\left(\frac{a}{b}\right) = 1 - \frac{1}{b},
\]
since $a/b \in E_k$ for all $k \geq b$. Let $a/b$ and $c/d$ be any successive fractions in $E_n$ and consider the fraction $(a+c)/(b+d) \in E_{n+1}$. Since
\[
\Phi(x) = \phi_1(x) + \cdots + \phi_{n-1}(x)
\]
is a linear function on the interval $[a/b, c/d]$, we get
\[
\Phi\left(\frac{a+c}{b+d}\right) = 1 - \frac{1}{b} + \frac{1/b - 1/d}{c/d - a/b} \left(\frac{a+c}{b+d} - \frac{a}{b}\right) = 1 - \frac{2}{b+d}.
\]
Hence
\[
f\left(\frac{a+c}{b+d}\right) = \Phi\left(\frac{a+c}{b+d}\right) + \phi_n\left(\frac{a+c}{b+d}\right) = 1 - \frac{1}{b+d},
\]
by definition. Therefore, (3.1) holds for any rational point in the interval $[0, 1]$.

Let $m$ be any positive integer. Since the piecewise linear function $\Phi_m(x)$ takes the value less than 1 at any point belonging to $E_{m+1}$, as is already seen above, it follows clearly that $\Phi_m(x) < 1$ for all $x \in [0, 1]$. Thus the series
\[
\sum_{n=1}^\infty \phi_n(x)
\]
converges at every irrational point $x$. Let $a/b$ and $c/d$ be any adjacent fractions in $E_{m+1}$ satisfying $a/b < x < c/d$. We then have
\[
\Phi_m(x) \geq \min\left\{\Phi_m\left(\frac{a}{b}\right), \Phi_m\left(\frac{c}{d}\right)\right\}
= \min\left\{1 - \frac{1}{b}, 1 - \frac{1}{d}\right\}.
\]
Since $x$ is irrational, both $b$ and $d$ must diverge to $\infty$ as $m \to \infty$; so, $f(x) = 1$, as required."
Continuous Functions,"Let $c_1, c_2, \ldots, c_n$ and $\lambda_1, \lambda_2, \ldots, \lambda_n$ be real numbers with 
$\lambda_j \neq \lambda_k$ for any $j \neq k$. Show that $c_1 = c_2 = \cdots = c_n = 0$ if
\[
\sum_{k=1}^n c_k \exp(\lambda_k i x)
\]
converges to $0$ as $x \to \infty$.","Put
\[
f(x) = \sum_{k=1}^n c_k \exp(\lambda_k i x).
\]
For any $\epsilon > 0$ we can find a sufficiently large integer $N$ satisfying $|f(x)| < \epsilon$ for all $x$ greater than $N$. For each $1 \leq k \leq n$ we have
\[
\frac{1}{T} \int_T^{2T} f(x) \exp(-\lambda_k i x) \, dx
= c_k + \frac{1}{T} \sum_{\ell \neq k} c_\ell \int_T^{2T} \exp((\lambda_\ell - \lambda_k) i x) \, dx
\]
\[
= c_k + \frac{1}{T} \sum_{\ell \neq k} \frac{\exp(2(\lambda_\ell - \lambda_k) i T) - \exp((\lambda_\ell - \lambda_k) i T)}{(\lambda_\ell - \lambda_k) i}.
\]

Therefore
\[
|c_k| \leq \frac{1}{T} \int_T^{2T} |f(x)| \, dx + \frac{2}{T} \sum_{\ell \neq k} \frac{|c_\ell|}{|\lambda_\ell - \lambda_k|}
< \epsilon + O\left(\frac{1}{T}\right)
\]
for any $T > N$, which implies $c_k = 0$ since $\epsilon$ is arbitrary."
Diﬀerentiation,"Suppose that all roots of the algebraic equation 
\[
x^n + a_{n-1} x^{n-1} + \cdots + a_0 = 0
\]
have negative real parts and that $f \in C^n(0, \infty)$. Show that if
\[
f^{(n)}(x) + a_{n-1} f^{(n-1)}(x) + \cdots + a_0 f(x)
\]
converges to $0$ as $x \to \infty$, then $f^{(k)}(x)$ also converges to $0$ as $x \to \infty$ for any $0 \leq k \leq n$.","We first consider the case $n = 1$ and assume that $f(x) \in C^1(0, \infty)$ is a complex-valued function. Suppose that $f'(x) + z f(x)$ converges to $0$ as $x \to \infty$ where $z$ is a complex number with $\Re z > 0$. Differentiating $g(x) = e^{z x} f(x)$ we have 
\[
g'(x) = e^{z x} \big(f'(x) + z f(x)\big);
\]
therefore $g'(x) e^{-z x}$ converges to $0$ as $x \to \infty$. This means that for any $\epsilon > 0$ there exists an $x_\epsilon$ satisfying $|g'(x)| e^{-z x} < \epsilon$ for any $x$ greater than $x_\epsilon$. Hence 
\[
e^{z x} |f(x)| = |g(x)| \leq |g(x_\epsilon)| + \int_{x_\epsilon}^x |g'(t)| dt.
\]
Since the last expression is equal to 
\[
e^{z x_\epsilon} |f(x_\epsilon)| + \epsilon \int_{x_\epsilon}^x e^{z t} dt,
\]
we have 
\[
|f(x)| \leq e^{z(x_\epsilon - x)} |f(x_\epsilon)| + \frac{\epsilon}{\Re z} e^{z x}.
\]
Thus $|f(x)| < 2\epsilon / \Re z$ for all sufficiently large $x$.

We prove the $n+1$ case by assuming the $n$ case. Let $-\xi$ be a root of 
\[
x^{n+1} + a_n x^n + \cdots + a_0 = 0
\]
with $\Re \xi > 0$. Since this polynomial is written as 
\[
(x + \xi)\big(x^n + b_{n-1}x^{n-1} + \cdots + b_0\big),
\]
we get 
\[
f^{(n+1)}(x) + a_n f^{(n)}(x) + \cdots + a_0 f(x) = \phi'(x) + \xi \phi(x)
\]
where 
\[
\phi(x) = f^{(n)}(x) + b_{n-1}f^{(n-1)}(x) + \cdots + b_0 f(x).
\]
By the hypothesis $\phi(x)$ converges to $0$ and hence so does $f^{(k)}(x)$ as $x \to \infty$ for each integer $0 \leq k \leq n$. Clearly $f^{(n+1)}(x)$ converges to $0$ as well."
Diﬀerentiation,"Show that any $f \in C^2(\mathbb{R})$ satisfies the inequality
\[
\left( \sup_{x \in \mathbb{R}} |f'(x)| \right)^2 \leq 2 \sup_{x \in \mathbb{R}} |f(x)| \cdot \sup_{x \in \mathbb{R}} |f''(x)|.
\]
Prove moreover that the constant $2$ on the right-hand side cannot in general be replaced by any smaller number.","Put
\[
\alpha = \sup_{x \in \mathbb{R}} |f(x)| \quad \text{and} \quad \beta = \sup_{x \in \mathbb{R}} |f''(x)|.
\]
We may of course assume that both $\alpha$ and $\beta$ are finite. If $\beta$ vanishes, then $\alpha$ is finite if and only if $f(x)$ vanishes everywhere; therefore we can also assume that $\beta$ is positive. For any $x$ and positive $y$ it follows from Taylor's formula that there is a $\xi_{x,y}$ satisfying
\[
f(x+y) = f(x) + f'(x)y + f''(\xi_{x,y}) \frac{y^2}{2}.
\]
Therefore we have
\[
f(x+y) - f(x-y) = 2f'(x)y + \left(f''(\xi_{x,y}) - f''(\xi_{x,-y})\right) \frac{y^2}{2},
\]
which implies that
\[
2|f'(x)|y = |f(x+y) - f(x-y) + \left(f''(\xi_{x,y}) - f''(\xi_{x,-y})\right) \frac{y^2}{2}|
\leq 2\alpha + \beta y^2.
\]
Thus we get
\[
\sup_{x \in \mathbb{R}} |f'(x)| \leq \frac{\alpha}{y} + \frac{\beta y}{2},
\]
where the right-hand side attains its minimum $\sqrt{2\alpha \beta}$ at $y = \sqrt{2\alpha / \beta}$.

To see that 2 is the best possible constant we first define an even step function
\[
\phi''(x) = 
\begin{cases} 
0 & \text{for } |x| > 2, \\
1 & \text{for } 1 \leq |x| \leq 2, \\
-1 & \text{for } |x| < 1.
\end{cases}
\]
Then
\[
\phi'(x) = \int_{-2}^x \phi''(t) \, dt
\]
is an odd piecewise linear continuous function and in turn
\[
\phi(x) = \int_{-2}^x \phi'(t) \, dt - \frac{1}{2}
\]
is an even $C^1$-function. The maxima of $|\phi(x)|$, $|\phi'(x)|$ and $|\phi''(x)|$ are clearly $1/2$, $1$, and $1$ respectively. The equality certainly holds in this example. However $\phi$ does not belong to $C^2(\mathbb{R})$. To tide over this difficulty it suffices to transform $\phi''$ slightly to a continuous one in the neighborhood of the discontinuity points of $\phi''$ so that its influence on $\phi$ and $\phi'$ becomes arbitrarily small."
Diﬀerentiation,"Let $Q_n(x)$ be a polynomial with real coefficients of degree $n$ and $M$ be the maximum of $|Q_n(x)|$ on the interval $[-1, 1]$. Show that
\[
\sqrt{1 - x^2} \, |Q'_n(x)| \leq nM
\]
for any $-1 \leq x \leq 1$. Show next that
\[
|Q'_n(x)| \leq n^2 M
\]
for any $-1 \leq x \leq 1$.","The proof is based on Cheney (1966), p. 89--91. We first show that any polynomial $Q(x)$ with complex coefficients of degree $n-1$ satisfies the inequality
\[
\max_{-1 \leq x \leq 1} |Q(x)| \leq n \max_{-1 \leq x \leq 1} \sqrt{1-x^2} |Q(x)|.
\]
Let $M$ denote the right-hand side. If $x^2 \leq 1 - 1/n^2$, then clearly $|Q(x)| \leq M$. Hence we can assume that 
\[
|x| > \sqrt{1 - 1/n^2} > \cos\frac{\pi}{2n}.
\]

The $n$th Chebyshev polynomial $T_n(x)$ of the first kind (See Chapter 15) is factorized as
\[
T_n(x) = (x-\xi_1) \cdots (x-\xi_n)
\]
where 
\[
\xi_k = \cos\frac{2k-1}{2n}\pi
\]
for $k = 1, 2, \ldots, n$. The Lagrange interpolation polynomial for $Q$ with nodes $\xi_1, \ldots, \xi_n$ is
\[
\sum_{k=1}^n \frac{Q(\xi_k)}{T_n'(\xi_k)} \cdot \frac{T_n(x)}{x-\xi_k} = \frac{1}{n}\sum_{k=1}^n (-1)^{k-1}Q(\xi_k) \sqrt{1-\xi_k^2} \frac{T_n(x)}{x-\xi_k},
\]
which is a polynomial of degree less than $n$; hence it coincides with the polynomial $Q(x)$. Using the fact that $\mathrm{sgn}(x-\xi_k)$ is independent of $k$ in view of $\xi_1 < |x| \leq 1$, we get
\[
|Q(x)| \leq \frac{M}{n^2} \sum_{k=1}^n \left| \frac{T_n(x)}{x-\xi_k} \right| = \frac{M}{n^2} \left| T_n'(x) \right|.
\]

Since 
\[
|T_n'(\cos\theta)| = n \frac{|\sin n\theta|}{|\sin\theta|} \leq n^2,
\]
we get $|Q(x)| \leq M$, as required.

By the substitution $x = \cos\theta$, our inequality is equivalent to
\[
\max_\theta |Q(\cos\theta)| \leq n \max_\theta |\sin\theta Q(\cos\theta)|,
\]
which is valid for all polynomial $Q$ with complex coefficients of degree $n-1$. Let $S(\theta)$ be a linear combination over $\mathbb{C}$ of $1, \cos\theta, \cos 2\theta, \ldots, \cos n\theta$ and $\sin\theta, \sin 2\theta, \ldots, \sin n\theta$. For any $\omega$ and $\theta$ we put
\[
S_0(\theta) = \frac{S(\omega+\theta) - S(\omega-\theta)}{2}.
\]
Since $S_0(\theta)$ is an odd function, this is a linear combination of $1, \sin\theta, \ldots, \sin n\theta$ only. Thus $S_0(\theta)/\sin\theta$ is a polynomial in $\cos\theta$ of degree less than $n$, since $\sin k\theta / \sin\theta$ can be expressed as a polynomial in $\cos\theta$ of degree $k-1$. Applying our inequality to this polynomial in $\cos\theta$, we have
\[
\max_\theta \left| \frac{S_0(\theta)}{\sin\theta} \right| \leq n \max_\theta |S_0(\theta)| \leq n \max_\theta |S(\theta)|.
\]

Therefore
\[
\lim_{\theta \to 0} \frac{S_0(\theta)}{\sin\theta} = \lim_{\theta \to 0} S_0'(\theta) = S'(\omega),
\]
which implies that
\[
\max_\theta |S'(\theta)| \leq n \max_\theta |S(\theta)|.
\]
This is called Bernstein's inequality (1912b).

Two inequalities stated in the problem can be solved by using Bernstein's inequality. Let $P(x)$ be any polynomial with complex coefficients of degree $n$. Then $P(\cos\theta)$ is a linear combination of $1, \cos\theta, \ldots, \cos n\theta$ and it follows from Bernstein's inequality that
\[
\max_{-1 \leq x \leq 1} \sqrt{1-x^2} |P'(x)| = \max_\theta |\sin\theta P'(\cos\theta)| \leq n \max_\theta |P(\cos\theta)| = nM.
\]

Therefore, since $P'(x)$ is a polynomial of degree $n-1$, we get
\[
\max_{-1 \leq x \leq 1} |P'(x)| \leq n \max_{-1 \leq x \leq 1} \sqrt{1-x^2} |P'(x)| \leq n^2 M.
\]
This completes the proof."
Diﬀerentiation,"Suppose that $f \in C^\infty(\mathbb{R})$ satisfies $f(0)f'(0) \geq 0$ and that $f(x)$ converges to 0 as $x \to \infty$. Show then that there exists an increasing sequence $0 \leq x_1 < x_2 < x_3 < \cdots$ satisfying
\[
f^{(n)}(x_n) = 0.
\]","Suppose first that $f'(x)$ is positive for all $x \geq 0$. Then $f(x)$ is strictly monotone increasing and $f(0) \geq 0$ because $f'(0)$ is positive. This is a contradiction because $f(x)$ converges to 0 as $x \to \infty$. Similarly, we get a contradiction if $f'(x)$ is negative for all $x \geq 0$. Hence there exists at least one point $x_1 \geq 0$ at which $f'(x)$ vanishes.

Suppose now that we could find $n$ points $x_1 < \cdots < x_n$ satisfying $f^{(k)}(x_k) = 0$ for $1 \leq k \leq n$. If $f^{(n+1)}(x)$ is positive for any $x > x_n$, then clearly $f^{(n)}(x) \geq f^{(n)}(x_n + 1) > 0$ for any $x \geq x_n + 1$, since $f^{(n)}(x_n) = 0$. Thus we have
\[
f(x) \geq \frac{1}{n!} f^{(n)}(x_n + 1)x^n + (\text{some polynomial of degree less than } n),
\]
contrary to the assumption that $f(x)$ converges to 0. Similarly, we would have a contradiction if $f^{(n+1)}(x)$ is negative for any $x > x_n$. Hence there exists at least one point $x_{n+1}$ greater than $x_n$ satisfying $f^{(n+1)}(x_{n+1}) = 0$."
Diﬀerentiation,"Show that any $f \in C^{n+1}[0, 1]$ satisfies
\[
\max_{0 \leq x \leq 1} \left| f^{(n+1)}(x) \right| \geq 4^n n!
\]
if $f(0) = f'(0) = \cdots = f^{(n)}(0) = f'(1) = \cdots = f^{(n)}(1) = 0$ and $f(1) = 1$.","Let
\[
P(x) = x^n + a_{n-1}x^{n-1} + \cdots + a_0
\]
be any polynomial with real coefficients. Integrating by parts repeatedly we have
\[
\int_0^1 P(x) f^{(n+1)}(x) \, dx = -\int_0^1 P'(x) f^{(n)}(x) \, dx = \cdots = (-1)^n \int_0^1 P^{(n)}(x) f'(x) \, dx = (-1)^n n!,
\]
where we used \(f(1) = 1\). Now taking \(P\) as the polynomial attaining the minimum in \textbf{Problem 5.6}, we get
\[
n! = \left| \int_0^1 P(x) f^{(n+1)}(x) \, dx \right| \leq \max_{0 \leq x \leq 1} \left| f^{(n+1)}(x) \right| \int_0^1 |P(x)| \, dx = \frac{1}{4^n} \max_{0 \leq x \leq 1} \left| f^{(n+1)}(x) \right|.
\]"
Diﬀerentiation,"Show that the maximum of $\left| Q^{(n)}(x) \right|$ over $[-1, 1]$ is equal to $2^n n!$, where
\[
Q(x) = (1 - x^2)^n.
\]","By Cauchy's integral formula we get
\[
Q^{(n)}(x) = \frac{n!}{2\pi i} \int_C \frac{(1 - z^2)^n}{(z - x)^{n+1}} \, dz
\]
where \(C\) is an oriented circle centered at \(z = x\) with radius \(r > 0\). Hence putting
\[
z = x + re^{i\theta}
\]
for \(0 \leq \theta < 2\pi\) we obtain
\[
Q^{(n)}(x) = \frac{n!}{2\pi} \int_0^{2\pi} \frac{(1 - (x + re^{i\theta})^2)^n}{r^{n+1} e^{(n+1)i\theta}} re^{i\theta} \, d\theta
= \frac{n!}{2\pi} \int_0^{2\pi} \frac{(1 - (x + re^{i\theta})^2)^n}{re^{i\theta}} \, d\theta.
\]

The expression in the parentheses can be written as
\[
\left( \frac{1 - x^2}{r} - r \right) \cos\theta - 2x - i \left( \frac{1 - x^2}{r} + r \right) \sin\theta.
\]

We now take \(r = \sqrt{1 - x^2}\) for \(|x| < 1\) so that \(|P^{(n)}(x)| \leq 2^n n!\). This inequality clearly holds for \(x = 1\) and \(x = -1\)."
Diﬀerentiation,"Define the piecewise linear function
\[
g(x) =
\begin{cases}
x & \text{for } 0 \leq x < 1/2, \\
1 - x & \text{for } 1/2 \leq x < 1,
\end{cases}
\]
and extend it to $\mathbb{R}$ periodically. Show that
\[
T(x) = \sum_{n=0}^\infty \frac{1}{2^n} g(2^n x)
\]
is continuous but nowhere differentiable.","The continuity of $T(x)$ is obvious since it is defined as the series of continuous functions converging uniformly. To show the non-differentiability it suffices to consider any point $x$ in the interval $(0, 1]$ since $T(x)$ is periodic with period 1.

We first consider any point $x$ which can be expressed in the form $k/2^m$ with some odd integer $k$ and non-negative integer $m$. For any integer $n \geq m$ put $h_n = 1/2^n$ for brevity. Then for any integer $\ell$ in $[0, n]$ there are no integers nor half-integers in the interval $(2^\ell x, 2^\ell(x + h_n))$. For if $2^\ell x < p/2 < 2^\ell x + 2^\ell h_n$ for some integer $p$, then we would have $2^n x = k 2^{n-m} < 2^{n-\ell-1} p < k 2^{n-m} + 1$, a contradiction. This means that $g(x)$ is a linear function having the slope 1 or $-1$ on this subinterval. Hence
\[
\frac{T(x + h_n) - T(x)}{h_n} = \sum_{\ell=0}^\infty \frac{g(2^\ell (x + h_n)) - g(2^\ell x)}{2^\ell h_n} = \sum_{\ell=0}^{n-1} \frac{g(2^\ell x + 2^\ell h_n) - g(2^\ell x)}{2^{\ell-n}}
\]
is a finite sum of 1 or $-1$, which does not converge as $n \to \infty$.

Next consider any point $x$ for which $2^n x$ is not an integer for all positive integer $n$. Since $2^n x$ is not an integer, we can find two positive numbers $h_n$ and $h_n'$ satisfying $[2^n x] = 2^n (x - h_n')$ and $[2^n x] + 1 = 2^n (x + h_n)$. Note that $h_n + h_n' = 2^{-n}$. Then for any integer $\ell$ in $[0, n]$ there are no integers nor half-integers in the interval $(2^\ell (x - h_n'), 2^\ell (x + h_n))$. For if $p/2$ were contained in this interval for some integer $p$, then we have $[2^n x] < 2^{n-\ell-1} p < [2^n x] + 1$, a contradiction. Therefore
\[
\frac{T(x + h_n) - T(x - h_n')}{h_n + h_n'} = \sum_{\ell=0}^\infty \frac{g(2^\ell (x + h_n)) - g(2^\ell (x - h_n'))}{2^\ell (h_n + h_n')} = \sum_{\ell=0}^{n-1} \frac{g(2^\ell x + 2^\ell h_n) - g(2^\ell x - 2^\ell h_n')}{2^{\ell-n}}
\]
is a finite sum of 1 or $-1$, which does not converge as $n \to \infty$."
Diﬀerentiation,"Suppose that $f \in C^1(0, \infty)$ is positive. Then show that for an arbitrary constant $a > 1$,
\[
\liminf_{x \to \infty} \frac{f'(x)}{(f(x))^a} \leq 0.
\]","Suppose, contrary to the assertion, that there are positive numbers $\delta$ and $x_0$ satisfying
\[
\delta < \frac{f'(x)}{(f(x))^a}
\]
for any $x > x_0$. Then integrating from $x_0$ to $x$ we have
\[
\delta(x - x_0) < \int_{x_0}^x \frac{f'(t)}{(f(t))^a} dt = \frac{1}{a-1} \left( \frac{1}{(f(x_0))^{a-1}} - \frac{1}{(f(x))^{a-1}} \right).
\]
Thus
\[
\frac{1}{(f(x_0))^{a-1}} > \frac{1}{(f(x))^{a-1}} + (a-1)\delta(x - x_0) > (a-1)\delta(x - x_0),
\]
where the right-hand side diverges to $\infty$ together with $x$, giving a contradiction."
Diﬀerentiation,"Suppose that $f \in C^2(0, \infty)$ converges to $\alpha$ as $x \to \infty$ and that 
\[
f''(x) + \lambda f'(x)
\]
is bounded above for some constant $\lambda$. Then show that $f'(x)$ converges to $0$ as $x \to \infty$.","The proof is substantially due to Hardy and Littlewood (1914). We use Taylor’s formula with the integral remainder term
\[
f(x+y) = f(x) + yf'(x) + y^2 \int_0^1 (1-t)f''(x+yt) dt,
\]
valid for any \(x > 1\) and \(|y| < 1\). Now let us consider the integral on the right-hand side with \(f''\) replaced by \(f'\). By the mean value theorem there is a \(\xi_{x,y}\) between \(0\) and \(y\) satisfying
\[
y^2 \int_0^1 (1-t)f'(x+yt) dt = \int_x^{x+y} f(s) ds - yf(x) = y f(x+\xi_{x,y}) - y f(x).
\]
Since there exists a positive constant \(K\) satisfying \(f''(x) + \lambda f'(x) \leq K\), we have
\[
f(x+y) - f(x) - y f'(x) + \lambda y f(x+\xi_{x,y}) - \lambda y f(x) = y^2 \int_0^1 (1-t) (f''(x+yt) + \lambda f'(x+yt)) dt \leq Ky^2 \int_0^1 (1-t) dt = \frac{K}{2}y^2.
\]

For the case in which \(0 < y < 1\) we get
\[
f'(x) \geq \frac{f(x+y) - f(x)}{y} + \lambda f(x+\xi_{x,y}) - \lambda f(x) - \frac{K}{2}y.
\]
Making \(x \to \infty\) we thus have
\[
\liminf_{x \to \infty} f'(x) \geq -\frac{K}{2}y.
\]
Therefore \(\liminf_{x \to \infty} f'(x)\) must be \(\geq 0\) because \(y\) is arbitrary.

Similarly, for the case in which \(-1 < y < 0\),
\[
f'(x) \leq \frac{f(x) - f(x-|y|)}{|y|} + \lambda f(x+\xi_{x,y}) - \lambda f(x) + \frac{K}{2}|y|,
\]
which implies that
\[
\limsup_{x \to \infty} f'(x) \leq \frac{K}{2}|y|.
\]
So that \(\limsup_{x \to \infty} f'(x)\) is \(\leq 0\) because \(y\) is arbitrary. Therefore \(f'(x)\) converges to \(0\) as \(x \to \infty\)."
Integration,"Suppose that \(f \in C[0,1]\) and \(g \in C(\mathbb{R})\) is a periodic function with period 1. Show then that
\[
\lim_{n \to \infty} \int_0^1 f(x) g(nx) \, dx = \int_0^1 f(x) \, dx \int_0^1 g(x) \, dx.
\]","Without loss of generality we can replace \( g(x) \) by \( g(x) + c \) for any constant \( c \); so we can assume that \( g(x) \) is positive. By the periodicity of \( g \)
\[
\int_0^1 f(x) g(nx) \, dx = \frac{1}{n} \int_0^n f\left(\frac{y}{n}\right) g(y) \, dy
= \frac{1}{n} \sum_{k=0}^{n-1} \int_k^{k+1} f\left(\frac{y}{n}\right) g(y) \, dy
= \frac{1}{n} \sum_{k=0}^{n-1} \int_0^1 f\left(\frac{k+s}{n}\right) g(s) \, ds.
\]

Applying the first mean value theorem to each integral on the right-hand side, the above expression can be written as the product of \( \int_0^1 g(s) \, ds \) times
\[
\frac{1}{n} \sum_{k=0}^{n-1} f\left(\frac{k+s_k}{n}\right)
\]
for some \( s_k \) in the interval \( (0, 1) \), which is the Riemann sum converging to \( \int_0^1 f(x) \, dx \) as \( n \to \infty \)."
Integration,"Find an example of a sequence of continuous functions \(\{f_n\}\) defined on the interval \([0, 1]\) such that \(0 \leq f_n(x) \leq 1\),
\[
\int_0^1 f_n(x) \, dx \to 0 \quad \text{as } n \to \infty,
\]
and that \(\{f_n(x)\}\) does not converge at any point \(x\) in \([0, 1]\).","Divide the unit interval \([0, 1]\) into \(m \geq 3\) equal subintervals. For each subinterval let \(I'\) and \(I''\) be the left and right neighboring subinterval respectively with \(I'\) or \(I''\) empty at endpoints. We associate with \(I\) the trapezoidal function \(g_I(x)\) such that \(g_I(x) = 1\) on \(I\), \(g_I(x) = 0\) on \([0, 1] \setminus (I \cup I' \cup I'')\) and that \(g_I(x)\) is linear on \(I'\) and on \(I''\). We thus have \(m\) continuous functions \(g_I\) for each \(m\). We call \(I\) the support of \(g_I\). Arranging the \(g_I\)'s in a line in any manner for \(m = 3, 4, \dots\) we define a sequence of continuous functions \(\{f_n(x)\}\). It is clear that
\[
a_n = \int_0^1 f_n(x) \, dx \leq \frac{2}{m}
\]
if \(f_n = g_I\) and the length of \(I\) is equal to \(1/m\). Since \(m \to \infty\) together with \(n\), it follows that \(a_n\) converges to \(0\) as \(n \to \infty\).

For any point \(x\) in \([0, 1]\) there are infinitely many cases in which \(x\) is contained in some support; in other words, \(f_n(x) = 1\) for infinitely many \(n\)'s. On the other hand, there are also infinitely many cases in which \(x\) is contained in neither the support nor its neighbors; in other words, \(f_n(x) = 0\) for infinitely many \(n\)'s. Hence \(\{f_n(x)\}\) does not converge at any point \(x\)."
Integration,"Show that
\[
\max_{0 \leq x \leq 1} |f'(x)| \geq 4 \int_0^1 |f(x)| \, dx
\]
for any \(f \in C^1[0, 1]\) satisfying \(f(0) = f(1) = 0\). Prove moreover that 4 cannot be replaced by any larger constant.","\[
\text{Let } g(x) \text{ be any one of the four functions } f(x), -f(x), f(1-x) \text{ and } -f(1-x).
\]
\[
\text{Let } \alpha \text{ be the maximum of } |g'(x)| \text{ on the interval } [0,1], \text{ which also equals to the maximum of } |f'(x)| \text{ on } [0,1].
\]
\[
\text{We can assume } \alpha > 0; \text{ otherwise, } f(x) \text{ would identically vanish.}
\]
\[
\text{Suppose now that there is a point } x_0 \in (0,1) \text{ satisfying } g(x_0) > \alpha x_0.
\]
\[
\text{By the mean value theorem, there exists } \xi \in (0,x_0) \text{ satisfying } g(\xi) = g'(\xi)x_0 > \alpha x_0.
\]
\[
\text{However, this implies } g'(\xi) > \alpha, \text{ contrary to the definition of } \alpha.
\]
\[
\text{We thus have } g(x) \leq \alpha x \text{ for any } 0 < x < 1 \text{ so that } |f(x)| \leq \alpha \max\{x, 1-x\}.
\]
\[
\text{Hence,}
\]
\[
\int_0^1 |f(x)|dx \leq \alpha \int_0^1 \max\{x, 1-x\}dx = \frac{\alpha}{4}.
\]
\[
\text{The equality does not occur since the function } \max\{x, 1-x\} \text{ is not in } C^1\text{-class.}
\]
\[
\text{However, we can modify it slightly to become a continuously differentiable one in the neighborhood of } x = 1/2
\]
\[
\text{so that the difference between } \int_0^1 |f(x)|dx \text{ and } \alpha/4 \text{ becomes sufficiently small.}
\]"
Integration,"For any positive integer \( n \) show that
\[
\int_0^1 |f(x)|^n |f'(x)| \, dx \leq \frac{1}{n+1} \int_0^1 |f'(x)|^{n+1} \, dx
\]
holds for any \( f \in C^1[0, 1] \) satisfying \( f(0) = 0 \). Verify that the equality holds if and only if \( f(x) \) is a linear function.","We introduce the auxiliary function
\[
\phi(x) = \frac{x^n}{n+1} \int_0^x \lvert f'(s) \rvert^{n+1} \, ds - \int_0^x \lvert f(s) \rvert^n \lvert f'(s) \rvert \, ds.
\]
Obviously $\phi(0) = 0$ and
\[
\phi'(x) = \frac{nx^{n-1}}{n+1} \int_0^x \lvert f'(s) \rvert^{n+1} \, ds + \frac{x^n}{n+1} \lvert f'(x) \rvert^{n+1} - \lvert f(x) \rvert^n \lvert f'(x) \rvert.
\]
Applying Hölder's inequality to $1$ and $\lvert f'(x) \rvert$, we get
\[
\lvert f(x) \rvert = \int_0^x \lvert f'(s) \rvert \, ds \leq \left( \int_0^x 1 \cdot \lvert f'(s) \rvert^{n+1} \, ds \right)^{1/(n+1)} \cdot \left( \int_0^x 1 \, ds \right)^{n/(n+1)},
\]
or
\[
\lvert f(x) \rvert \leq x^{n/(n+1)} \left( \int_0^x \lvert f'(s) \rvert^{n+1} \, ds \right)^{1/(n+1)},
\]
or
\[
\int_0^x \lvert f'(s) \rvert^{n+1} \, ds \geq \frac{\lvert f(x) \rvert^{n+1}}{x^n}.
\]
Hence
\[
\phi'(x) \geq \frac{n}{n+1} \cdot \frac{\lvert f(x) \rvert^{n+1}}{x} + \frac{x^n}{n+1} \lvert f'(x) \rvert^{n+1} - \lvert f(x) \rvert^n \lvert f'(x) \rvert.
\]
The right-hand side multiplied by $(n+1)x$ is expressed as
\[
\sigma(\lvert f(x) \rvert, x \lvert f'(x) \rvert),
\]
where
\[
\sigma(a, b) = na^{n+1} + b^{n+1} - (n+1)a^n b.
\]
Since $\sigma(a, 0) \geq 0$, we can assume $b > 0$. Put $t = a/b \geq 0$ for brevity. Then
\[
\frac{\sigma(a, b)}{b^{n+1}} = nt^{n+1} + 1 - (n+1)t^n
\]
attains its minimum $0$ at $t = 1$, which implies that $\phi(x)$ is monotone increasing. In particular we have $\phi(1) \geq 0$, as required.

The equality occurs in Hölder's inequality if and only if $f(x)$ is linear. In this case the equality actually occurs in the inequality in question."
Integration,"Suppose that both \( f(x) \) and \( g(x) \) are monotone increasing continuous functions defined on \([0, 1]\). Show that
\[
\int_0^1 f(x) \, dx \int_0^1 g(x) \, dx \leq \int_0^1 f(x) g(x) \, dx.
\]","It suffices to show that
\[
\int_0^1 f(x) \phi(x) \, dx \geq 0
\]
where
\[
\phi(x) = g(x) - \int_0^1 g(t) \, dt.
\]
By the mean value theorem there is a $\xi \in (0, 1)$ satisfying
\[
g(\xi) = \int_0^1 g(x) \, dx.
\]
Since $\phi(x) \leq 0$ for $0 \leq x \leq \xi$ and $\phi(x) \geq 0$ for $\xi \leq x \leq 1$, we have
\[
\int_0^1 f(x) \phi(x) \, dx = \int_0^\xi f(x) \phi(x) \, dx + \int_\xi^1 f(x) \phi(x) \, dx
\]
\[
\geq f(\xi) \int_0^\xi \phi(x) \, dx + f(\xi) \int_\xi^1 \phi(x) \, dx
\]
\[
= f(\xi) \int_0^1 \phi(x) \, dx = 0.
\]"
Integration,"Show that the minimum of the integral
\[
\int_0^1 \left| x^n + a_1 x^{n-1} + \cdots + a_n \right| dx
\]
as \( a_1, a_2, \ldots, a_n \) range over all real numbers, is equal to \( 4^{-n} \).","For a given polynomial
\[
A(x) = x^n + a_1 x^{n-1} + \cdots + a_n
\]
we define
\[
B(x) = A\left(\frac{x+2}{4}\right),
\]
so that
\[
B(x) = \frac{x^n}{4^n} + a_1' x^{n-1} + \cdots + a_n'
\]
with some real numbers $a_1', \ldots, a_n'$. Putting further
\[
Q(x) = \int_0^x B(s) \, ds
\]
we obtain
\[
Q(x) = \frac{x^{n+1}}{4^n (n+1)} + a_1'' x^n + \cdots + a_n'' x
\]
with some real numbers $a_1'', \ldots, a_n''$. Applying the same method as in the proof of \textbf{Problem 3.2} to $Q(x)$ with the same notations, we get
\[
\frac{4(n+1)}{4^n (n+1)} \sum_{k=0}^{n+1} (-1)^k Q(\alpha_k) \leq \sum_{k=0}^n |Q(\alpha_k) - Q(\alpha_{k+1})| \tag{5.6}
\]
where
\[
\alpha_k = 2 \cos \frac{k \pi}{n+1}.
\]
Therefore, by using (5.4) and (5.5) in (5.6),
\[
\frac{1}{4^{n-1}} \leq \sum_{k=0}^n \int_{\alpha_{k+1}}^{\alpha_k} |B(s)| \, ds = \int_{-2}^2 |B(s)| \, ds = 4 \int_0^1 |A(x)| \, dx.
\]"
Integration,"For any \( f \in C^1[0, 1] \) show that
\[
\sum_{k=1}^n f\left(\frac{k}{n}\right) - n \int_0^1 f(x) \, dx
\]
converges to
\[
\frac{f(1) - f(0)}{2}
\]
as \( n \to \infty \).","We have
\[
\sum_{k=1}^n f\left(\frac{k}{n}\right) - n \int_0^1 f(x) \, dx = n \sum_{k=1}^n \int_{(k-1)/n}^{k/n} \left( f\left(\frac{k}{n}\right) - f(x) \right) dx.
\]
By the mean value theorem there is a $\xi_{k,x}$ in each open interval $((k-1)/n, k/n)$ satisfying
\[
f\left(\frac{k}{n}\right) - f(x) = f'(\xi_{k,x})\left(\frac{k}{n} - x\right).
\]
Since $f'(x)$ is uniformly continuous on $[0, 1]$, we have for any $\epsilon > 0$
\[
\left| f'(\xi_{k,x}) - f'\left(\frac{k}{n}\right) \right| < \epsilon
\]
for all $1 \leq k \leq n$ and for all $x$ in the interval $[(k-1)/n, k/n)$ on taking $n$ sufficiently large. Hence
\[
\left| S_n - \frac{1}{2n} \sum_{k=1}^n f'\left(\frac{k}{n}\right) \right| < \frac{\epsilon}{2}
\]
where $S_n$ is the expression in the problem. Since $\epsilon$ is arbitrary, we get
\[
\lim_{n \to \infty} S_n = \frac{1}{2} \int_0^1 f'(x) \, dx = \frac{f(1) - f(0)}{2}.
\]"
Integration,"Put \( \phi(x) = 1 / \sqrt{1 + |x|} \) and \( \alpha_{j,n} = (j - 1/2)3^{-n} \) for \( 1 \leq j \leq 3^n, n \geq 0 \).

Show that there exist \( \{c_{j,n}\}_{1 \leq j \leq 3^n, n \geq 0} \subset (0, 1) \) and \( \{\lambda_n\} \) with \( \lambda_n > 3^{5n+1} \) such that
\[
\max_{0 \leq x \leq 1} \sum_{k=0}^n \psi_k(x) < 1 - \frac{1}{n+2}
]\]
and
\[
\sum_{k=0}^n \psi_k(\alpha_{j,n}) > 1 - \frac{1}{n+1} 
\]
for any \( 1 \leq j \leq 3^n \) and \( n \geq 0 \), where
\[
\psi_k(x) = \sum_{j=1}^{3^k} c_{j,k} \phi(\lambda_k(x - \alpha_{j,k})).
\]","We show this by induction on \( n \). When \( n = 0 \), (5.1) and (5.2) clearly hold for any \( c_{1,0} \in (0, 1/2) \) and \( \lambda_0 > 3^0 = 1 \).

We next suppose that (5.1) holds for \( n = m - 1 \); that is,
\[
\max_{0 \leq x \leq 1} \sum_{k=0}^{m-1} \psi_k(x) < 1 - \frac{1}{m+1}.
\]
Then we can take a constant \( c_{\ell,m} \) in the interval \( (0, 1) \) satisfying
\[
1 - \frac{1}{m+1} < c_{\ell,m} + \sum_{k=0}^{m-1} \psi_k(\alpha_{\ell,m}) < 1 - \frac{1}{m+2} \tag{5.7}
\]
for each \( 1 \leq \ell \leq 3^m \) and with \( c_{\ell,m} \)'s we define
\[
\psi(x, \lambda) = \sum_{\ell=1}^{3^m} c_{\ell,m} \phi\left(\lambda \left(x - \alpha_{\ell,m}\right)\right).
\]
Since
\[
\lim_{\lambda \to \infty} \psi(x, \lambda) = 
\begin{cases} 
c_{\ell,m}, & \text{if } x = \alpha_{\ell,m}, \\
0, & \text{otherwise},
\end{cases}
\]
we can take a sufficiently large \( \lambda_m > 3^{5m+1} \) such that \( \psi(\alpha_{\ell,m}, \lambda_m) = \psi_k(\alpha_{\ell,m}) \) is sufficiently close to \( c_{\ell,m} \) for all \( 1 \leq \ell \leq 3^m \). Substituting this in (5.7), we conclude that
\[
1 - \frac{1}{m+1} < \sum_{k=0}^{m-1} \psi_k(\alpha_{\ell,m}) < 1 - \frac{1}{m+2}.
\]
This shows that (5.1) and (5.2) hold for \( n = m \).

By the property (a), we see that the partial sums of
\[
\Phi(x) = \sum_{n=0}^\infty \psi_n(x)
\]
form a monotone increasing sequence, and so the series converges pointwise. Moreover, \( 0 < \Phi(x) \leq 1 \) for any \( x \in [0, 1] \) and \( \Phi(x) = 1 \) for any \( x \in A \), where
\[
A = \{\alpha_{\ell,m} ; 1 \leq \ell \leq 3^m, m \geq 0\}
\]
is a dense subset of the interval \( [0,1] \).

To see that \( \Phi(x) \) is not constant on any subinterval of \( [0,1] \), we put \( \beta_{k,n} = k3^{-n} \) for \( 0 \leq k \leq 3^n \) and
\[
B = \{\beta_{k,n} ; 0 \leq k \leq 3^n, n \geq 0\}.
\]
\( B \) is also a dense subset of \( [0,1] \) satisfying \( A \cap B = \emptyset \). Since
\[
\left| \alpha_{j,m} - \beta_{k,n} \right| = \left| \frac{2j - 1}{2 \cdot 3^m} - \frac{k}{3^n} \right| \geq \frac{1}{2 \cdot 3^m},
\]
for any integers \( j, k, m, n \), we have
\[
\psi_m(\beta_{k,n}) = \sum_{j=1}^{3^m} c_{j,m} \phi\left(\lambda_m \left(\beta_{k,n} - \alpha_{j,m}\right)\right) < 3^m \sqrt{\frac{2 \cdot 3^m}{\lambda_m}} < \frac{1}{3^m}.
\]
Therefore,
\[
\Phi(\beta_{k,n}) = \sum_{m=0}^{n-1} \psi_m(\beta_{k,n}) + \sum_{m=n}^\infty \psi_m(\beta_{k,n})
< 1 - \frac{1}{n+1} + \sum_{m=n}^\infty \frac{1}{3^m},
\]
which is less than 1 for \( n \) large enough. We thus have \( \Phi(x) < 1 \) for any \( x \in B \).
"
Integration,"Put \( \phi(x) = 1 / \sqrt{1 + |x|} \) and \( \alpha_{j,n} = (j - 1/2)3^{-n} \) for \( 1 \leq j \leq 3^n, n \geq 0 \).
Show that
\[
\Psi(x) = \sum_{n=0}^\infty \int_0^x \psi_n(t) \, dt
\]
is differentiable and satisfies \( \Psi'(x) = \sum_{n=0}^\infty \psi_n(x) \) for any \( x \in (0, 1) \).","Let \( V \) be the set of all positive continuous functions \( f \) defined on \( \mathbb{R} \) satisfying \( \sigma_{a,b}(f) < 4 \min\{f(a), f(b)\} \) for any \( a \neq b \), where
\[
\sigma_{a,b}(f) = \frac{1}{b-a} \int_a^b f(x) \, dx.
\]

We first show that \( \phi \in V \). Since \( \phi \) is an even function, it suffices to consider only two cases: \( 0 \leq a < b \) and \( a < 0 < b \). If \( 0 \leq a < b \), then
\[
\sigma_{a,b}(\phi) = \frac{2}{\sqrt{1+a} + \sqrt{1+b}} < 2 \phi(b).
\]
If \( a < 0 < b \), then
\[
\sigma_{a,b}(\phi) = \frac{2}{|a| + b} \left( \sqrt{1+|a|} + \sqrt{1+b} - 2 \right),
\]
which is less than
\[
\frac{4}{c} \left( \sqrt{1+c} - 1 \right) = \frac{4}{\sqrt{1+c} + 1} < 4 \phi(c),
\]
where \( c = \max\{|a|, b\} \), as required.

Note that \( V \) forms a positive cone; that is, if \( f_1 \) and \( f_2 \) belong to \( V \), then \( c_1 f_1 + c_2 f_2 \) also belongs to \( V \) for any positive constants \( c_1 \) and \( c_2 \). Moreover, for any \( f \in V \) and \( \lambda > 0 \), the function \( \tilde{f} \) defined by \( \tilde{f}(x) = f(\lambda x) \) belongs to \( V \) in view of
\[
\sigma_{a,b}(\tilde{f}) = \sigma_{\lambda a, \lambda b}(f) < 4 \min\{f(\lambda a), f(\lambda b)\} = 4 \min\{\tilde{f}(a), \tilde{f}(b)\}.
\]
This means that every \( \psi_k(x) \) defined above belongs to \( V \).

Hence, noting that
\[
\left| \int_0^x \psi_n(t) \, dt \right| \leq 4 \psi_n(0),
\]
we infer that the series \( \Psi(x) \) given in (5.3) converges absolutely and uniformly for \( 0 \leq x \leq 1 \). Therefore, \( \Psi(x) \) is continuous on the interval \( [0, 1] \). Moreover, for an arbitrary fixed \( x \in (0, 1) \) and any \( \epsilon > 0 \), we can take an integer \( N = N(x, \epsilon) \) satisfying
\[
\sum_{n=N}^\infty \psi_n(x) < \epsilon.
\]

We then take a sufficiently small number \( \delta > 0 \) such that
\[
|\psi_k(\xi) - \psi_k(x)| < \frac{\epsilon}{N}
\]
for any integer \( 0 \leq k < N \) and any \( \xi \) with \( |x-\xi| < \delta \). Thus, for any \( 0 < |h| < \delta \), we obtain
\[
\left| \frac{\Psi(x+h) - \Psi(x)}{h} - \sum_{n=0}^\infty \psi_n(x) \right| = \left| \sum_{n=0}^\infty \frac{1}{h} \int_x^{x+h} \left( \psi_n(t) - \psi_n(x) \right) \, dt \right|
\leq \epsilon + 2 \sum_{n=N}^\infty \psi_n(x) < 3\epsilon,
\]
which implies that \( \Psi(x) \) is differentiable and satisfies
\[
\Psi'(x) = \sum_{n=0}^\infty \psi_n(x)
\]
for any \( x \in (0,1) \).
"
Integration,"Put \( \phi(x) = 1 / \sqrt{1 + |x|} \) and \( \alpha_{j,n} = (j - 1/2)3^{-n} \) for \( 1 \leq j \leq 3^n, n \geq 0 \).
Using the function \( \Psi \), construct an example of everywhere differentiable but nowhere monotone functions.","Finally we construct an example of everywhere differentiable but nowhere monotone functions. Let
\[
f(x) = \Psi(x) - \Psi\left(x - \frac{1}{6}\right)
\]
for \( \frac{1}{6} < x < 1 \). Since \( \alpha_{j,n} - \frac{1}{6} = \beta_{k,n} \) for any \( k = j - \frac{3^{n-1} + 1}{2} \geq 0 \), we have
\[
f'(\alpha_{j,n}) = \Phi(\alpha_{j,n}) - \Phi(\beta_{k,n}) > 0.
\]
Similarly, since \( \beta_{k,n} - \frac{1}{6} = \alpha_{j,n} \) for any \( j = k - \frac{3^{n-1} - 1}{2} \geq 1 \), we get
\[
f'(\beta_{k,n}) = \Phi(\beta_{k,n}) - \Phi(\alpha_{j,n}) < 0.
\]
Thus, \( A \) and \( B \) being dense subsets of \( [0,1] \), \( f(x) \) is nowhere monotone.
"
Integration,"Prove that
\[
\sum_{n=1}^m \frac{\sin n\theta}{n} < \int_0^\pi \frac{\sin x}{x} \, dx = 1.8519\ldots
\]
for any positive integer \( m \) and any \( \theta \) in \( [0, \pi] \). Show moreover that the constant on the right-hand side cannot be replaced by any smaller number.","As is already seen in \textbf{Solution 1.7}, the maximal values in the interval \([0, \pi]\) of the function
\[
S_m(\theta) = \sin \theta + \frac{\sin 2\theta}{2} + \cdots + \frac{\sin m\theta}{m}
\]
are attained at \([(m+1)/2]\) points: \(\pi/(m+1), 3\pi/(m+1), \ldots\). Since
\[
S'_m(\theta) = \frac{1}{2} \sin 2(m+1)\theta \cot \theta - \cos^2(2(m+1)\theta),
\]
we get
\[
S_m(\beta) - S_m(\alpha) \leq \int_{\alpha/2}^{\beta/2} \sin 2(m+1)\theta \cot \theta \, d\theta
\]
for any \(0 < \alpha < \beta < \pi\). By the substitution \(s = 2(m+1)\theta - 2\ell\pi\) with
\[
\alpha = \frac{2\ell - 1}{m+1}\pi \quad \text{and} \quad \beta = \frac{2\ell + 1}{m+1}\pi,
\]
the above integral on the right-hand side can be written as
\[
\frac{1}{2(m+1)} \int_0^\pi \sin s \left( \cot \frac{2\ell\pi + s}{2(m+1)} - \cot \frac{2\ell\pi - s}{2(m+1)} \right) ds.
\]
Since the function \(\cot s\) is strictly monotone decreasing in the interval \((0, \pi/2)\), this integral is clearly negative. This implies that the maximum of \(S_m(\theta)\) on the interval \([0, \pi]\) is attained at \(\theta = \pi/(m+1)\). Moreover, we have
\[
S_{m+1}\left(\frac{\pi}{m+2}\right) > S_{m+1}\left(\frac{\pi}{m+1}\right) = S_m\left(\frac{\pi}{m+1}\right).
\]
Therefore \(\{S_m(\pi/(m+1))\}\) is a strictly monotone increasing sequence and
\[
S_m\left(\frac{\pi}{m+1}\right) = \frac{\pi}{m+1} \sum_{n=1}^{m+1} \frac{m+1}{n\pi} \sin \frac{n\pi}{m+1}
\]
converges to the integral
\[
\int_0^\pi \frac{\sin x}{x} dx \quad \text{as } m \to \infty.
\]"
Improper Integrals,"Show that
\[
\int_0^1 x^{-x} dx = \frac{1}{1^1} + \frac{1}{2^2} + \frac{1}{3^3} + \cdots + \frac{1}{n^n} + \cdots.
\]","We have
\[
\int_0^1 x^{-x} \, dx = \int_0^1 e^{-x \log x} \, dx = \sum_{n=0}^\infty \frac{(-1)^n}{n!} \int_0^1 x^n \log^n x \, dx,
\]
where the termwise integration is allowed since the series
\[
\sum_{n=0}^\infty \frac{(-1)^n}{n!} x^n \log^n x
\]
converges uniformly on the interval \( (0, 1] \). Also, by the substitution \( x = e^{-s} \), we get
\[
\int_0^1 x^n \log^n x \, dx = (-1)^n \int_0^\infty e^{-(n+1)s} s^n \, ds = (-1)^n \frac{n!}{n^{n}},
\]
from the definition of the Gamma function.
"
Improper Integrals,"Show that
\[
\lim_{n \to \infty} \sqrt{n} \int_{-\infty}^\infty \frac{dx}{(1 + x^2)^n} = \sqrt{\pi}.
\]","We divide \( (-\infty, \infty) \) into three parts as
\[
(-\infty, -n^{-1/3}) \cup [-n^{-1/3}, n^{-1/3}] \cup (n^{-1/3}, \infty),
\]
which we denote by \( A_1, A_2, A_3 \) respectively. For \( k = 1, 2, 3 \) put
\[
I_k = \sqrt{n} \int_{A_k} \frac{dx}{(1 + x^2)^n}.
\]

By the substitution \( t = \sqrt{n} x \) we have
\[
I_2 = \sqrt{n} \int_{A_2} \exp\left(-n \log \left(1 + x^2\right)\right) dx
= \int_{-n^{1/6}}^{n^{1/6}} \exp\left(-n \log \left(1 + \frac{t^2}{n}\right)\right) dt.
\]

Since
\[
n \log \left(1 + \frac{t^2}{n}\right) = t^2 + O\left(\frac{1}{n^{1/3}}\right)
\]
uniformly in \( |t| \leq n^{1/6} \), we obtain
\[
I_2 = \left(1 + O\left(\frac{1}{n^{1/3}}\right)\right) \left(\sqrt{\pi} - \int_{-\infty}^{-n^{1/6}} e^{-t^2} dt - \int_{n^{1/6}}^{\infty} e^{-t^2} dt\right)
= \sqrt{\pi} + O\left(\frac{1}{n^{1/3}}\right)
\]
as \( n \to \infty \). On the other hand, it follows that
\[
0 < I_1 + I_3 < \frac{\sqrt{n}}{(1 + n^{-2/3})^{n-1}} \int_{-\infty}^\infty \frac{dx}{1 + x^2}
\]
and the right-hand side converges to 0 as \( n \to \infty \)."
Improper Integrals,"Show that
\[
\int_{0}^\infty \frac{e^{-x/s - 1/x}}{x} \, dx \sim \log s
\]
as \( s \to \infty \).","Let \( I(s) \) be the improper integral in the problem, which is invariant under the substitution \( t = s / x \); hence
\[
I(s) = 2 \int_{\sqrt{s}}^\infty \frac{e^{-x/s - 1/x}}{x} dx.
\]

Since \( e^{-1/x} = 1 + O(s^{-1/2}) \) uniformly in \( x \geq \sqrt{s} \) as \( s \to \infty \), we have
\[
I(s) = 2 \left( 1 + O\left(\frac{1}{\sqrt{s}}\right) \right) \int_{\sqrt{s}}^\infty \frac{e^{-x/s}}{x} dx
= 2 \left( 1 + O\left(\frac{1}{\sqrt{s}}\right) \right) \int_{s^{-1/2}}^\infty \frac{e^{-t}}{t} dt.
\]

Integrating the last integral by parts, we get
\[
\int_{s^{-1/2}}^\infty \frac{e^{-t}}{t} dt = \left[ e^{-t} \log t \right]_{t = s^{-1/2}}^{t = \infty} + \int_{s^{-1/2}}^\infty e^{-t} \log t \, dt
= \frac{1}{2} \log s + O(1),
\]
hence \( I(s) = \log s + O(1) \) as \( s \to \infty \).
"
Improper Integrals,"Suppose that \( g \in C[0, \infty) \) is monotone decreasing and that
\[
\int_{0}^\infty g(x) \, dx
\]
converges. (Note that \( g(x) \geq 0 \) for any \( x \geq 0 \).) Show then that
\[
\lim_{h \to 0^+} h \sum_{n=1}^\infty f(nh) = \int_{0}^\infty f(x) \, dx
\]
for any \( f \in C[0, \infty) \) satisfying \( |f(x)| \leq g(x) \) for all \( x \geq 0 \).","For any \( \epsilon > 0 \) there exists a sufficiently large number \( L > 1 \) satisfying
\[
\int_{L-1}^\infty g(x) \, dx < \epsilon.
\]
For any \( h \) in the interval \( (0, 1) \), take a positive integer \( N \) satisfying
\[
Nh \leq L < (N+1)h.
\]
Then
\[
nh \in \left( \frac{Ln}{N+1}, \frac{Ln}{N} \right) \subset \left( \frac{L(n-1)}{N}, \frac{Ln}{N} \right)
\]
for any integer \( 1 \leq n \leq N \). Thus the Riemann sum
\[
\frac{L}{N} \sum_{n=1}^N f(nh)
\]
converges to the integral
\[
\int_0^L f(x) \, dx \quad \text{as } N \to \infty.
\]
Since \( N \to \infty \) as \( h \to 0^+ \), there exists a sufficiently small \( h_0 > 0 \) such that
\[
\left| \frac{L}{N} \sum_{n=1}^N f(nh) - \int_0^L f(x) \, dx \right| < \epsilon \tag{6.1}
\]
and
\[
\frac{1}{N} \int_0^\infty g(x) \, dx < \epsilon
\]
for any \( 0 < h < h_0 \). On the other hand, we have
\[
\left| \left( \frac{L}{N} - h \right) \sum_{n=1}^N f(nh) \right| \leq \left( \frac{L}{N} - h \right) \sum_{n=1}^N g(nh) \leq \frac{h}{N} \sum_{n=1}^N g(nh).
\]
By the monotonicity of \( g(x) \), the right-hand side is less than or equal to
\[
\frac{1}{N} \int_0^L g(x) \, dx,
\]
which is clearly less than \( \epsilon \), whence
\[
\left| \left( \frac{L}{N} - h \right) \sum_{n=1}^N f(nh) \right| < \epsilon. \tag{6.2}
\]

Moreover
\[
h \left| \sum_{n > N} f(nh) \right| \leq h \sum_{n > N} g(nh) \leq \int_{L-1}^\infty g(x) \, dx < \epsilon; \tag{6.3}
\]
therefore, by (6.1), (6.2), and (6.3),
\[
\left| h \sum_{n=1}^\infty f(nh) - \int_0^\infty f(x) \, dx \right| < 3\epsilon + \int_L^\infty |f(x)| \, dx < 4\epsilon.
\]
Since \( \epsilon \) is arbitrary, this completes the proof.
"
Improper Integrals,"For \( s > 0 \), compute
\[
\int_{0}^\infty e^{-(x-s/x)^2} \, dx.
\]","Let \( f(s) \) be the integral in the problem. It is easily seen that
\[
e^{-4s} f(s) = \int_0^\infty \exp\left(-\left(x + \frac{s}{x}\right)^2\right) dx.
\]
Differentiating both sides, we get
\[
\left(f'(s) - 4f(s)\right)e^{-4s} = -2 \int_0^\infty \left(1 + \frac{s}{x^2}\right) \exp\left(-\left(x + \frac{s}{x}\right)^2\right) dx,
\]
where the differentiation under the integral sign is allowed (see the introduction of Chapter 11). Then, by the substitution \( t = x - s/x \), we have
\[
f'(s) - 4f(s) = -2 \int_0^\infty \left(1 + \frac{s}{x^2}\right) \exp\left(-\left(x + \frac{s}{x}\right)^2\right) dx
= -2 \int_{-\infty}^\infty e^{-t^2} dt = -2 \sqrt{\pi}.
\]

Solving this linear differential equation, we get
\[
f(s) = c e^{4s} + \frac{\sqrt{\pi}}{2}
\]
for some constant \( c \). Since \( 0 < f(s) < (\sqrt{\pi}/2)e^{2s} \), we have \( c = 0 \); hence
\[
f(s) = \frac{\sqrt{\pi}}{2}.
\]"
Improper Integrals,"Suppose that \( f \in C(\mathbb{R}) \) and that \( \int_{-\infty}^\infty |f(x)| \, dx \) converges. Show then that
\[
\lim_{n \to \infty} \int_{-\infty}^\infty f(x) |\sin nx| \, dx = \frac{2}{\pi} \int_{-\infty}^\infty f(x) \, dx.
\]","For any \( \epsilon > 0 \), we can take a sufficiently large number \( L \) satisfying
\[
\int_{-\infty}^{-L\pi} |f(x)| dx + \int_{L\pi}^\infty |f(x)| dx < \epsilon. \tag{6.4}
\]
On the other hand,
\[
\int_{-L\pi}^{L\pi} |f(x)| \sin nx| dx = \pi \int_{-L}^L f(\pi t) |\sin n\pi t| dt
= \pi \sum_{k=-L}^{L-1} \int_0^1 f(\pi s + k\pi) |\sin n\pi s| ds.
\]
Applying the result in \textbf{Problem 5.1}, the right-hand side converges to
\[
\pi \sum_{k=-L}^{L-1} \int_0^1 f(\pi s + k\pi) ds \int_0^1 \sin n\pi s ds
\]
as \( n \to \infty \), which is
\[
\frac{2}{\pi} \int_{-L\pi}^{L\pi} f(t) dt.
\]
Whence
\[
\int_{-L\pi}^{L\pi} f(x)|\sin nx| dx \to \frac{2}{\pi} \int_{-L\pi}^{L\pi} f(t) dt \quad (n \to \infty). \tag{6.5}
\]
Therefore, by (6.4) and (6.5), we have
\[
\limsup_{n \to \infty} \left| \int_{-\infty}^\infty f(x)|\sin nx| dx - \frac{2}{\pi} \int_{-\infty}^\infty f(t) dt \right| \leq 2\epsilon.
\]
Since \( \epsilon \) is arbitrary, this completes the proof."
Improper Integrals,"Show that
\[
\frac{7}{12} - \gamma = \int_{0}^\infty \frac{\{x\}^2 (1 - \{x\})^2}{(1 + x)^5} \, dx,
\]
where \( \gamma \) is Euler's constant and \( \{x\} \) denotes the fractional part of \( x \).","Let \( I \) be the integral on the right-hand side of the problem. We have
\[
I = \sum_{k=1}^\infty \int_{k-1}^k \frac{\{x\}^2 (1 - \{x\})^2}{(1+x)^5} dx
= \int_0^1 t^2 (1-t)^2 H_5(t) dt,
\]
where for any integer \( m > 1 \) we write
\[
H_m(x) = \sum_{k=1}^\infty \frac{1}{(x+k)^m}.
\]
Then, by integration by parts, we get
\[
I = \int_0^1 t (1-t) \left( \frac{1}{2} - t \right) H_4(t) dt
= \int_0^1 \left( \frac{1}{6} - t (1-t) \right) H_3(t) dt.
\]
Since
\[
\int_0^1 H_3(t) dt = -\frac{1}{2} \left( H_2(1) - H_2(0) \right) = \frac{1}{2},
\]
it follows that
\[
I = \frac{1}{12} + \int_0^1 \left( t - \frac{1}{2} \right) H_2(t) dt;
\]
therefore
\[
I = \frac{1}{12} + \lim_{n \to \infty} \sum_{k=1}^n \int_0^1 \frac{t - 1/2}{(t+k)^2} dt
= \frac{1}{12} - \lim_{n \to \infty} \sum_{k=1}^n \left( \frac{1}{2k} + \frac{1}{2(k+1)} - \log \frac{k+1}{k} \right),
\]
which is equal to \( 7/12 - \gamma \) from the definition of Euler’s constant."
Improper Integrals,"Show that a rational function \( R(x) \) satisfies
\[
\int_{-\infty}^\infty f(R(x)) \, dx = \int_{-\infty}^\infty f(x) \, dx
\]
for all piecewise continuous functions \( f(x) \) such that \( \int_{-\infty}^\infty f(x) \, dx \) exists, if and only if
\[
R(x) = \pm \left( x - \alpha_0 - \sum_{k=1}^m \frac{c_k}{x - \alpha_k} \right)
\]
for some non-negative integer \( m \), real constants \( \alpha_0, \alpha_1, \ldots, \alpha_m \) with \( \alpha_1 < \cdots < \alpha_m \), and positive constants \( c_1, \ldots, c_m \).","The proof is based on Szegő (1934).

For any \( \epsilon \) in the interval \( (0, 1) \), we can find a positive constant \( M \) satisfying
\[
|R(x) - R(x_0)| \leq |R'(x_0)| \epsilon + M \epsilon^2
\]
for any \( |x - x_0| \leq \epsilon \) unless \( x_0 \) is a real pole of \( R(x) \). Let \( f_\epsilon(x) \) be the piecewise continuous function defined by
\[
f_\epsilon(x) =
\begin{cases} 
1 & \text{if } |x - R(x_0)| \leq |R'(x_0)| \epsilon + M \epsilon^2, \\
0 & \text{otherwise}.
\end{cases}
\]

Then we have
\[
2 \epsilon (|R'(x_0)| + M \epsilon) = \int_{-\infty}^\infty f_\epsilon(x) \, dx \geq \int_{x_0-\epsilon}^{x_0+\epsilon} f_\epsilon(R(x)) \, dx = 2 \epsilon;
\]
hence, \( \epsilon \) being arbitrary, we have \( |R'(x_0)| \geq 1 \). This means that \( R(x) \) maps each interval divided by real poles of \( R(x) \), say \( \alpha_1 < \alpha_2 < \cdots < \alpha_m \), bijectively on \( \mathbb{R} \). Of course \( m = 0 \) corresponds to the case in which \( R(x) \) has no real poles. The equation \( R(x) = s \) has a unique solution in each interval \( (-\infty, \alpha_1), (\alpha_1, \alpha_2), \ldots, (\alpha_m, \infty) \), say \( x_k(s) \), \( 0 \leq k \leq m \).

Next, let \( g_\epsilon(x) \) be the piecewise continuous function defined by
\[
g_\epsilon(x) =
\begin{cases} 
1 & \text{if } |x - s| \leq \epsilon, \\
0 & \text{otherwise}.
\end{cases}
\]

We then have
\[
2 \epsilon = \int_{-\infty}^\infty g_\epsilon(x) \, dx = \int_{-\infty}^\infty g_\epsilon(R(x)) \, dx, \tag{6.6}
\]
where the right-hand side is equal to the sum of lengths of \( m+1 \) intervals satisfying \( |R(x) - s| \leq \epsilon \). Each of these intervals contains exactly one \( x_k(s) \), and the length is
\[
\frac{2 \epsilon}{|R'(x_k(s))|} + O(\epsilon^2);
\]
so letting \( \epsilon \to 0^+ \) in (6.6), we get
\[
\sum_{k=0}^m \frac{1}{|R'(x_k(s))|} = 1. \tag{6.7}
\]
In the case $m = 0$ the function $R(x)$ maps $\mathbb{R}$ homeomorphically onto $\mathbb{R}$ and $R'(x_0(s)) = \pm 1$ implies that
\[
x_0'(s) = (R^{-1})'(s) = \frac{1}{R'(R^{-1}(s))} = \frac{1}{R'(x_0(s))} = \pm 1.
\]
Therefore $R(x) = \pm (x - a_0)$ for some constant $a_0$ for $m = 0$.

We hereafter assume that $m$ is a positive integer. Since $|R'(x_k(s))|$ diverges to $\infty$ as $|s| \to \infty$ for each $1 \leq k \leq m$, it follows from $(6.7)$ that either
\[
\begin{cases}
R'(x) \geq 1 \text{ on } (-\infty, \alpha_1) \cup (\alpha_m, \infty), \\
\lim_{s \to -\infty} x_0(s) = -\infty, \quad \lim_{s \to -\infty} x_m(s) = \alpha_m, \\
\lim_{s \to \infty} x_0(s) = \alpha_1, \quad \lim_{s \to \infty} x_m(s) = \infty,
\end{cases}
\]
or
\[
\begin{cases}
R'(x) \leq -1 \text{ on } (-\infty, \alpha_1) \cup (\alpha_m, \infty), \\
\lim_{s \to -\infty} x_0(s) = \alpha_1, \quad \lim_{s \to -\infty} x_m(s) = \infty, \\
\lim_{s \to \infty} x_0(s) = -\infty, \quad \lim_{s \to \infty} x_m(s) = \alpha_m,
\end{cases}
\]
holds. They are called the first and the second cases respectively.

The rational function $R(x)$ can now be represented as
\[
R(x) = P(x) - \sum_{k,\ell} \frac{c_{k,\ell}}{(x - \alpha_k)^{d_{k,\ell}}} + Q(x),
\]
where $P(x)$ is a polynomial of degree $r$ with some positive integer $r$, $c_{k,\ell}$ are non-zero real constants, $d_{k,\ell}$ are positive integers, and $Q(x)$ is a rational function having no real poles and converging to $0$ as $|x| \to \infty$ unless $Q(x)$ vanishes identically. Let $\beta x^r$ be the leading term of $P(x)$ where $\beta$ is a non-zero constant. Since $R'(x) \sim \beta r x^{r-1}$ as $|x| \to \infty$, we must have $r = 1$ and hence $\beta = \pm 1$ by $(6.7)$. Hence $P(x) = \pm (x - a_0)$ for some constant $a_0$.

We first treat the first case. Since $R'(x) = 1 + O(x^{-2})$ as $|x| \to \infty$ in this case, we have
\[
\sum_{k=0}^{m-1} \frac{1}{|R'(x_k(s))|} = O(s^{-2}) \quad \text{as } s \to \infty,
\]
\[
\sum_{k=1}^m \frac{1}{|R'(x_k(s))|} = O(s^{-2}) \quad \text{as } s \to -\infty. \tag{6.8}
\]

Let $d_k^*$ be the largest integer among $d_{k,\ell}$ and $c_k^*$ be the corresponding coefficient $c_{k,\ell}$ for each $0 \leq k \leq m$. Clearly $x_k(s) \to \alpha_k$ either as $s \to \infty$ or $s \to -\infty$ for $1 \leq k \leq m$. Then we have
\[
R(x) \sim -\frac{c_k^*}{(x - \alpha_k)^{d_k^*}} \quad \text{as } x \to \alpha_k,
\]
and in particular,
\[
s = R(x_k(s)) \sim -\frac{c_k^*}{(x_k(s) - \alpha_k)^{d_k^*}},
\]
whence solving in $x_k(s) - \alpha_k$ and substituting in
\[
R'(x_k(s)) \sim \frac{c_k^* d_k^*}{(x_k(s) - \alpha_k)^{d_k^*+1}} \quad \text{as } x \to \alpha_k,
\]
we obtain
\[
\frac{1}{|R'(x_k(s))|} \sim \frac{|c_k^*|^{1/d_k^*}}{d_k^*} \cdot \frac{1}{|s|^{1+1/d_k^*}}
\]
either as $s \to \infty$ or $s \to -\infty$. Therefore, in view of $(6.8)$, $d_k^* = 1$ and $R(x)$ can be written as
\[
R(x) = x - a_0 - \sum_{k=1}^m \frac{c_k}{x - \alpha_k} + Q(x) \tag{6.9}
\]
for some real constants $c_k$ in the first case. Since $R(x)$ is monotone increasing on $(\alpha_m, \infty)$ we have $c_m > 0$ and hence all coefficients $c_k$ must be positive; in particular,
\[
\sum_{k=0}^m \frac{1}{R'(x_k(s))} = 1. \tag{6.10}
\]

Differentiating $s = R(x_k(s))$ in $s$ and substituting in $(6.10)$, we get
\[
\sum_{k=0}^m x_k'(s) = 1,
\]
from which it follows that
\[
\sum_{k=0}^m x_k(s) = s + c'. 
\]
for some constant $c'$. Since $x_m(s) = s + a_0 + O(s^{-1})$ and $x_k(s) = \alpha_k + O(s^{-1})$ as $s \to \infty$ for each $0 \leq k < m$, we have
\[
\sum_{k=0}^m x_k(s) = s + \sum_{k=0}^m \alpha_k. \tag{6.11}
\]

The rest of the proof is devoted to showing the vanishing of $Q$. It may be interesting to find an easier real-analytic proof of this part.

Suppose, on the contrary, that $Q(x) \not\equiv 0$. Then, in view of $(6.9)$, we can write
\[
R(x) = \frac{V(x)}{U(x)} \quad \text{with} \quad U(x) = A(x) \prod_{k=1}^m (x - \alpha_k)
\]
where
\[
\begin{aligned}
A(x) &= x^{2p} - c''x^{2p-1} + O(x^{2p-2}), \\
V(x) &= x^{m+2p+1} - \left(c'' + \sum_{k=0}^m \alpha_k \right)x^{m+2p} + O(x^{m+2p-1})
\end{aligned}
\]
are polynomials with real coefficients for some positive integer $p$ and some constant $c''$. We can assume that $U(x)$ and $V(x)$ are relatively prime; that is, they have no common factor except for constants. Let $w_1, w_1', \ldots, w_p, w_p'$ be non-real zeros of $A(z)$. The algebraic equation
\[
V(z) = sU(z)
\]
has exactly $m + 1$ real simple roots $x_0(s), \ldots, x_m(s)$ and $2p$ non-real roots $z_1(s), \ldots, z_p(s), z_1'(s), \ldots, z_p'(s)$ counting with multiplicity for any real number $s$. Then we have from $(6.11)$
\[
\frac{z_1(s) + z_1'(s) + \cdots + z_p(s) + z_p'(s)}{s} = c''.
\]

Let $C$ be a circle enclosing all the zeros of $U(z)$. Take a sufficiently large $s$ such that
\[
s \min_{z \in C} |U(z)| > \max_{z \in C} |V(z)|
\]
and that $x_m(s)$ lies outside of $C$. Since $|V(z)| < s|U(z)|$ on $C$, it follows from Rouché's theorem that $V(z) = sU(z)$ has exactly $m + 2p$ roots inside of $C$, which are of course $x_1(s), \ldots, x_m(s)$ and $z_1(s), \ldots, z_p(s), z_1'(s), \ldots, z_p'(s)$. This implies that all the non-real roots are bounded as $s \to \infty$.
\[
V(z) - sU(z) \text{ is irreducible as a polynomial of two variables; that is, it cannot be expressed as the product of two polynomials none of which is constant.}
\]
We then consider \( x_k(s) \) and \( z_k(s) \) as function elements of the algebraic function uniquely determined by \( V(z) - sU(z) = 0 \). Since all solutions of \( V(z) - sU(z) = 0 \) are branches of the same algebraic function, it follows in particular that \( x_m(s) \) can be continued to \( z_1(s) \) along an arc on the Riemann surface of this algebraic function. This is a contradiction, since the relation \( (6.12) \) holds globally except for possible isolated singularities and since \( x_m(s) \) is a unique solution which is unbounded as \( s \to \infty \). Therefore \( p = 0 \) and hence \( Q(x) \) must vanish identically, completing the proof of the first part.

\textbf{The similar argument can be applied to the second case.}

Conversely, let
\[
R(x) = \pm \left( x - a_0 - \sum_{k=1}^m \frac{c_k}{x - \alpha_k} \right)
\]
for some real numbers \( a_0, \ldots, \alpha_m \) with \( \alpha_1 < \cdots < \alpha_m \) and some positive constants \( c_1, \ldots, c_m \). Then it is clear that \( \sum_{k=0}^m x_k'(s) = \pm 1 \) and hence
\[
\int_{-\infty}^\infty f(R(x)) dx = \int_{-\infty}^{\alpha_1} + \cdots + \int_{\alpha_m}^\infty f(R(x)) dx
\]
\[
= \pm \sum_{k=0}^m \int_{-\infty}^\infty f(s) x_k'(s) ds,
\]
which is equal to
\[
\int_{-\infty}^\infty f(x) dx. \quad \Box
\]
"
Improper Integrals,"Show that
\[
\gamma = \int_0^1 \frac{1 - \cos x}{x} \, dx - \int_1^\infty \frac{\cos x}{x} \, dx,
\]
where \( \gamma \) is Euler's constant.","The proof is essentially due to Gronwall (1918). Put
\[
c_n = \int_0^{n\pi} \frac{1 - \cos x}{x} dx - \log(n\pi)
\]
for any positive integer $n$. Then obviously
\[
c_n = \int_0^1 \frac{1 - \cos x}{x} dx - \int_1^{n\pi} \frac{\cos x}{x} dx,
\]
and we are to show that $c_n$ converges to Euler's constant $\gamma$ as $n \to \infty$.

By the substitution $x = 2n\pi s$ we have
\[
\int_0^{n\pi} \frac{1 - \cos x}{x} dx = \int_0^{1/2} \frac{1 - \cos 2n\pi s}{s} ds,
\]
which is equal to
\[
\pi \int_0^{1/2} \frac{1 - \cos 2n\pi s}{\sin \pi s} ds + \int_0^{1/2} \phi(s) ds - \int_0^{1/2} \phi(s) \cos 2n\pi s \, ds, \tag{6.13}
\]
where
\[
\phi(s) = \frac{1}{s} - \frac{\pi}{\sin \pi s}
\]
is a continuous function on the interval $[0, 1/2]$ if we define $\phi(0) = 0$. Hence by the remark after \textbf{Problem 5.1} the third integral in (6.13) converges to $0$ as $n \to \infty$. The second integral in (6.13) is equal to
\[
\left[ \log \frac{s}{\tan(\pi s / 2)} \right]_{s=0^+}^{s=1/2} = \log \pi - 2 \log 2.
\]
Finally, since it is easily verified that
\[
\frac{1 - \cos 2n\pi s}{\sin \pi s} = 2 \sum_{k=1}^n \sin(2k-1)\pi s,
\]
the first integral in (6.13) is equal to
\[
2\pi \sum_{k=1}^n \int_0^{1/2} \sin(2k-1)\pi s \, ds = \sum_{k=1}^n \frac{2}{2k-1},
\]
which is $\log n + 2 \log 2 + \gamma + o(1)$ as $n \to \infty$. Thus we obtain $c_n = \gamma + o(1)$, which completes the proof."
Series of Functions,"Show that
\[
\lim_{x \to 1^-} \sqrt{1-x} \sum_{n=1}^\infty x^{n^2} = \frac{\sqrt{\pi}}{2}.
\]","Applying \textbf{Problem 6.4} for \( f(x) = e^{-x^2} \), we have
\[
h \sum_{n=0}^\infty \exp(-n^2 h^2) \to \int_0^\infty e^{-x^2} dx = \frac{\sqrt{\pi}}{2}
\]
as \( h \to 0^+ \). By the substitution \( h = \sqrt{-\log x} \), it follows that \( h \) converges to \( 0^+ \) if and only if \( x \) converges to \( 1^- \); hence, using \( -\log x \sim 1 - x \) as \( x \to 1^- \), we obtain
\[
\lim_{x \to 1^-} \sqrt{1 - x} \sum_{n=0}^\infty x^{n^2}
= \lim_{x \to 1^-} \sqrt{-\log x} \sum_{n=0}^\infty x^{n^2}
= \lim_{h \to 0^+} h \sum_{n=0}^\infty \exp(-n^2 h^2) = \frac{\sqrt{\pi}}{2}.
\]"
Series of Functions,"Show that
\[
\lim_{x \to 1^-} (1-x)^2 \sum_{n=1}^\infty \frac{n x^n}{1 - x^n} = \frac{\pi^2}{6}.
\]","Applying \textbf{Problem 6.4} for \( f(x) = x / (e^x - 1) \), we get
\[
h \sum_{n=1}^\infty \frac{n h}{e^{n h} - 1} \to \int_0^\infty \frac{x \, dx}{e^x - 1} = \frac{\pi^2}{6}
\]
as \( h \to 0^+ \). By the substitution \( h = -\log x \), it holds that \( h \) converges to \( 0^+ \) if and only if \( x \) converges to \( 1^- \); hence, using \( -\log x \sim 1 - x \) as \( x \to 1^- \),
\[
\lim_{x \to 1^-} (1 - x)^2 \sum_{n=1}^\infty \frac{n x^n}{1 - x^n}
= \lim_{x \to 1^-} (\log x)^2 \sum_{n=1}^\infty \frac{n x^n}{1 - x^n}
= \lim_{h \to 0^+} h \sum_{n=1}^\infty \frac{n h}{e^{n h} - 1} = \frac{\pi^2}{6}.
\]
Note that the function \( f(x) = x / (e^x - 1) \) can be regarded as a continuous function on the interval \([0, \infty)\) if we define \( f(0) = 1 \)."
Series of Functions,"Suppose that a power series
\[
f(x) = \sum_{n=0}^\infty a_n x^n
\]
has the radius of convergence $\rho > 0$ and that $\sum_{n=0}^\infty a_n \rho^n$ converges to $\alpha$. Show that $f(x)$ converges to $\alpha$ as $x \to \rho^-$.","Replacing \( x \) by \( \rho x \), we can assume that \( \rho = 1 \) without loss of generality. Put
\[
s_n = a_0 + a_1 + \cdots + a_n.
\]
Since \( s_n \) converges to \( \alpha \) as \( n \to \infty \), for any \( \epsilon > 0 \), we can take a sufficiently large integer \( N \) satisfying
\[
|s_n - \alpha| < \epsilon
\]
for all integers \( n > N \). For \( 0 < x < 1 \), we have
\[
\frac{f(x)}{1-x} = \sum_{n=0}^\infty s_n x^n = \frac{\alpha}{1-x} + \sum_{n=0}^\infty (s_n - \alpha)x^n;
\]
therefore
\[
|f(x) - \alpha| \leq (1-x) \sum_{n=0}^N |s_n - \alpha| x^n + (1-x) \sum_{n>N} |s_n - \alpha| x^n
\]
\[
< (1-x) \sum_{n=0}^N |s_n - \alpha| + (1-x) \sum_{n=0}^\infty \epsilon x^n
\]
\[
= (1-x) \sum_{n=0}^N |s_n - \alpha| + \epsilon.
\]

The right-hand side can be \( < 2\epsilon \) by letting \( x \) be sufficiently close to \( 1^- \). This means that \( f(x) \) converges to \( \alpha \) as \( x \to 1^- \). \(\Box\)"
Series of Functions,"Suppose that a power series
\[
g(x) = \sum_{n=0}^\infty b_n x^n
\]
has the radius of convergence $\rho > 0$, all $b_n$ are positive, and that $\sum_{n=0}^\infty b_n \rho^n$ diverges to $\infty$. Then show that $f(x)/g(x)$ converges to $\alpha$ as $x \to \rho^-$ for any power series
\[
f(x) = \sum_{n=0}^\infty a_n x^n
\]
such that $a_n / b_n$ converges to $\alpha$ as $n \to \infty$.","As in the previous problem we can assume that $\rho = 1$. For any $\epsilon > 0$ there is a positive integer $N$ such that $|a_n - \alpha b_n| < \epsilon b_n$ for all $n$ greater than $N$. Since
\[
f(x) = \sum_{n=0}^\infty a_n x^n = \alpha g(x) + \sum_{n=0}^\infty (a_n - \alpha b_n) x^n,
\]
we have
\[
\left| \frac{f(x)}{g(x)} - \alpha \right| \leq \frac{1}{g(x)} \left( \sum_{n=0}^N |a_n - \alpha b_n| + \epsilon \sum_{n>N} b_n x^n \right)
\]
\[
< \frac{1}{g(x)} \left( \sum_{n=0}^N |a_n - \alpha b_n| + \epsilon \right). \tag{7.3}
\]

For any $0 < x < 1$. Now $g(x)$ diverges to $\infty$ as $x \to 1^-$, since
\[
\liminf_{x \to 1^-} g(x) \geq \sum_{k=0}^n b_k
\]
for any positive integer $n$. Hence the right-hand side of (7.3) can be smaller than $2\epsilon$ if we take $x$ sufficiently close to $1$. \qed"
Series of Functions,"Suppose that a power series 
\[
f(x) = \sum_{n=0}^\infty a_n x^n
\]
has the radius of convergence $\rho > 0$, $n a_n$ converges to $0$ as $n \to \infty$, and that $f(x)$ converges to $\alpha$ as $x \to \rho^-$. Show then that
\[
\sum_{n=0}^\infty a_n \rho^n = \alpha.
\]","As in the previous problems we can assume that $\rho = 1$. Put
\[
s_n = a_0 + a_1 + \cdots + a_n.
\]

For any $\epsilon > 0$ there is a positive integer $N$ such that $n|a_n| < \epsilon$ for all $n$ greater than $N$. For any $0 < x < 1$ and any $n > N$ we obtain
\[
|s_n - f(x)| \leq \sum_{k=1}^n |a_k|(1 - x^k) + \sum_{k > n} k|a_k| \frac{x^k}{k}
\]
\[
\leq (1 - x) \sum_{k=1}^n k|a_k| + \frac{\epsilon}{n(1-x)}.
\]

Substituting $x = 1 - 1/n$ we infer that
\[
\left| s_n - f\left(1 - \frac{1}{n}\right) \right| \leq \frac{|a_1| + 2|a_2| + \cdots + n|a_n|}{n} + \epsilon.
\]

Therefore the right-hand side can be smaller than $2\epsilon$ if we take $n$ sufficiently large. This means that $s_n$ converges to $\alpha$ as $n \to \infty$. "
Series of Functions,"Suppose that a power series 
\[
f(x) = \sum_{n=0}^\infty a_n x^n
\]
has the radius of convergence $1$, all $a_n$ are non-negative, and that $(1 - x)f(x)$ converges to $1$ as $x \to 1^-$. Show then that
\[
\lim_{n \to \infty} \frac{a_0 + a_1 + a_2 + \cdots + a_n}{n} = 1.
\]","It follows from the assumption that 
\[
(1 - x) \sum_{n=0}^\infty a_n x^{(k+1)n} = \frac{1}{1+x+\cdots+x^k}(1-x^{k+1}) \sum_{n=0}^\infty a_n \left(x^{k+1}\right)^n
\]
converges to 
\[
\frac{1}{k+1} = \int_0^1 t^k dt
\]
as $x \to 1^-$ for any non-negative integer $k$. Therefore we have 
\[
\lim_{x \to 1^-} (1 - x) \sum_{n=0}^\infty a_n x^n P(x^n) = \int_0^1 P(t) \, dt
\]
for any polynomial $P(t)$. 

We now introduce the discontinuous function $\phi(x)$ defined by 
\[
\phi(x) = 
\begin{cases} 
0 & \text{for } 0 \leq x < 1/e, \\
1/x & \text{for } 1/e \leq x \leq 1.
\end{cases}
\]
For any $\epsilon > 0$ we can find two continuous functions $\phi_\pm(x)$ defined on the interval $[0, 1]$ such that $\phi_-(x) \leq \phi(x) \leq \phi_+(x)$ and $\phi_+(x) - \phi_-(x) < \epsilon$ for any $x$ in $[0, 1]$. By Weierstrass' approximation theorem there are polynomials $P_\pm(x)$ satisfying $\lvert \phi_\pm(x) \pm \epsilon - P_\pm(x) \rvert < \epsilon$, respectively. Hence it follows that $P_-(x) < \phi(x) < P_+(x)$ and $P_+(x) - P_-(x) < 5\epsilon$.

Since $a_n$ are all non-negative, we obtain
\[
\sum_{n=0}^\infty a_n x^n P_-(x^n) \leq \sum_{n=0}^\infty a_n x^n \phi(x^n) \leq \sum_{n=0}^\infty a_n x^n P_+(x^n)
\]
for any $0 < x < 1$. If we put $x = e^{-1/N}$, then $x \to 1^-$ if and only if $N \to \infty$; hence, using $1 - e^{-1/N} \sim 1/N$, we have 
\[
\int_0^1 P_-(t) \, dt \leq \liminf_{N \to \infty} \frac{1}{N} \sum_{n=0}^N a_n
\]
and 
\[
\limsup_{N \to \infty} \frac{1}{N} \sum_{n=0}^N a_n \leq \int_0^1 P_+(t) \, dt.
\]
Therefore, since $P_+(x) - P_-(x) < 5\epsilon$ and 
\[
\int_0^1 \phi(x) dx = \int_{1/e}^1 \frac{dx}{x} = 1,
\]
it follows that 
\[
1 < \int_0^1 P_+(t) \, dt < \int_0^1 P_-(t) \, dt + 5\epsilon < 1 + 5\epsilon.
\]
Hence the sequence 
\[
\frac{1}{N} \sum_{n=0}^N a_n
\]
converges to $1$ as $N \to \infty$. 
"
Series of Functions,"Show that the series 
\[
f(x) = \sum_{n=0}^\infty e^{-n} \cos n^2 x
\]
is infinitely differentiable everywhere, but the Taylor series about $x = 0$
\[
\sum_{n=0}^\infty \frac{f^{(n)}(0)}{n!} x^n
\]
does not converge except for the origin.","By \( k \) times termwise differentiation of the given series we get
\[
\sum_{n=1}^\infty n^{2k} e^{-n} \mathcal{R}\left(i^k e^{in^2 x}\right),
\]
which clearly converges uniformly on \(\mathbb{R}\). Hence this series represents \( f^{(k)}(x) \); in particular,
\[
f^{(k)}(0) = \mathcal{R}(i^k) \sum_{n=1}^\infty n^{2k} e^{-n}.
\]

We thus have
\[
|f^{(2\ell)}(0)| \geq \left(\frac{4\ell}{e}\right)^{4\ell}
\]
for any positive integer \(\ell\) by looking at only the \(4\ell\)-th term. Let \(\rho\) be the radius of convergence of \(f(x)\). It then follows from Hadamard's formula (7.2) that
\[
\frac{1}{\rho} \geq \limsup_{\ell \to \infty} \left(\frac{(4\ell)^{4\ell}}{(2\ell)! e^{4\ell}}\right)^{1/(2\ell)}
\geq \limsup_{\ell \to \infty} \frac{(4\ell)^2}{2e^2\ell} = \infty,
\]
which means \(\rho = 0\)."
Series of Functions,Suppose that $f \in C^\infty(\mathbb{R})$ satisfies $f^{(n)}(x) \geq 0$ for any non-negative integer $n$ and for all $x \in \mathbb{R}$. Show that the Taylor series about $x = 0$ generated by $f$ converges for all $x$.,"It follows from Taylor’s formula about \( x = a \) with Lagrange’s remainder term that
\[
f(x) = \sum_{k=0}^n \frac{f^{(k)}(a)}{k!}(x-a)^k + \frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1}
\]
for some \(\xi\) between \(x\) and \(a\). If we take \(x = 2a > 0\), then
\[
f(2a) = \sum_{k=0}^n \frac{f^{(k)}(a)}{k!}a^k + \frac{f^{(n+1)}(\xi)}{(n+1)!}a^{n+1}
\geq \sum_{k=0}^n \frac{f^{(k)}(a)}{k!}a^k,
\]
which implies the convergence of the series
\[
\sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!}a^n.
\]

In particular, \( f^{(n)}(a)a^n / n! \) converges to 0 as \( n \to \infty \) for any \(a > 0\). Hence
\[
\left| f(x) - \sum_{k=0}^n \frac{f^{(k)}(0)}{k!}x^k \right|
= \frac{f^{(n+1)}(\xi)}{(n+1)!}|x|^{n+1}
\leq \frac{f^{(n+1)}(|x|)}{(n+1)!}|x|^{n+1} \to 0
\]
as \( n \to \infty \), since by assumption the derivative of \( f(x) \) of any order is monotone increasing for any \(x\).
"
Series of Functions,"Let $\{a_n\}$ be a monotone decreasing sequence converging to $0$. Show that the trigonometric series
\[
\sum_{n=1}^\infty a_n \sin n\theta
\]
converges uniformly on $\mathbb{R}$ if and only if $n a_n$ converges to $0$ as $n \to \infty$.","Suppose first that the given series converges uniformly on $\mathbb{R}$. For any $\epsilon > 0$, we can take a positive integer $N$ such that 
\[
\max_{\theta \in \mathbb{R}} \left| a_p \sin p \theta + a_{p+1} \sin (p+1)\theta + \cdots + a_q \sin q \theta \right| < \epsilon
\]
for any integers $q > p$ greater than $N$. We now choose $\theta = \pi/(4p)$ and $q = 2p$ so that $a_k \sin k \theta \geq 0$ for $p \leq k \leq 2p$. Then we have 
\[
\epsilon > \sum_{n=p}^{2p} a_n \sin \frac{n \pi}{4p} \geq \frac{1}{\sqrt{2}} \sum_{n=p}^{2p} a_n \geq \frac{p+1}{\sqrt{2}} a_{2p}.
\]
Hence 
\[
\max \{ 2p a_{2p}, (2p+1)a_{2p+1} \} \leq 2(p+1)a_{2p} < 2 \sqrt{2} \epsilon,
\]
and so $n a_n \to 0$ as $n \to \infty$.

Conversely, assume that $n a_n$ converges to $0$ as $n \to \infty$. For any $\epsilon > 0$, we can take a positive integer $N$ so that $n a_n < \epsilon$ for any integer $n$ greater than $N$. For any $\theta \in (0, \pi]$, let $m_\theta$ be a unique positive integer satisfying $\pi/(m+1) < \theta \leq \pi/m$. For any integers $q > p$ greater than $N$, let $S_0(\theta)$ and $S_1(\theta)$ be the sums of $a_n \sin n \theta$ from $n = p$ to $r$ and from $n = r+1$ to $q$ respectively, where 
\[
r = \min \{ q, p+m_\theta - 1 \}.
\]

For the sum $S_0(\theta)$, we use an almost trivial estimate $\sin x < x$ for $x > 0$ to obtain
\[
|S_0(\theta)| \leq \theta \sum_{n=p}^r n a_n < \theta m_\theta \epsilon \leq \pi \epsilon.
\]

Next, for the sum $S_1(\theta)$, we can assume $q \geq p + m_\theta$, so $r = p+m_\theta - 1$. By partial summation, we have 
\[
|S_1(\theta)| = \left| \sum_{n=r+1}^q a_n (\sigma_n - \sigma_{n-1}) \right| \leq a_{r+1}|\sigma_r| + a_q |\sigma_q| + (a_{r+1} - a_q) \max_{r < k < q} |\sigma_k|,
\]
where 
\[
\sigma_n = \sin \theta + \sin 2\theta + \cdots + \sin n\theta.
\]

Using Jordan's inequality $\pi \sin x \geq 2x$ valid on the interval $[0, \pi/2]$, we get 
\[
|\sigma_n| = \left| \frac{\cos \vartheta - \cos(2n+1)\vartheta}{\sin \vartheta} \right| \leq \frac{2}{\sin \vartheta} \leq \frac{\pi}{\vartheta} < m_\theta + 1,
\]
where $\theta = 2\vartheta$; therefore
\[
|S_1(\theta)| \leq (m_\theta + 1)(a_{r+1} + a_q + a_{r+1} - a_q) \leq 2(r+1)a_{r+1} < 2\epsilon.
\]

We then have 
\[
\left| \sum_{n=p}^q a_n \sin n \theta \right| \leq |S_0(\theta)| + |S_1(\theta)| < (\pi + 2)\epsilon,
\]
which holds uniformly on the interval $[0, \pi]$; whence on $\mathbb{R}$ by symmetry and periodicity.
"
Series of Functions,"Show that the trigonometric series
\[
\sum_{n=1}^\infty \frac{\sin n\theta}{n}
\]
converges uniformly to
\[
\frac{\pi - \theta}{2}
\]
on the interval $[\delta, 2\pi - \delta]$ for any $\delta > 0$.","The uniform convergence on any interval $[\delta, 2\pi - \delta]$ with $\delta > 0$ can be easily derived from Dirichlet's test (See Item 2 on p. 93). Thus it suffices to show that the limit function is $(\pi - \theta)/2$.

Put $\theta = 2\vartheta$ for brevity. Integrating the formula
\[
\frac{1}{2} + \sum_{n=1}^m \frac{\sin n\omega}{n} = \frac{\sin (2m+1)\vartheta}{2\sin \vartheta}
\]
from $0$ to $\omega \in [\delta, 2\pi - \delta]$, we obtain
\[
\frac{\omega}{2} + \sum_{n=1}^m \frac{\sin n\omega}{n} = \int_0^\omega \frac{\sin (2m+1)\vartheta}{\sin \vartheta} \, d\vartheta
= \int_0^\eta \frac{\sin (2m+1)\vartheta}{\sin \vartheta} \, d\vartheta,
\]
where $\omega = 2\eta$. We divide the last integral into two parts,
\[
\int_0^\eta \frac{\sin (2m+1)t}{t} \, dt + \int_0^\eta \phi(t) \sin (2m+1)t \, dt
\]
and call them $I_m(\eta)$ and $J_m(\eta)$ respectively, where
\[
\phi(t) = \frac{1}{\sin t} - \frac{1}{t}.
\]

Note that $\phi \in C^1(-\pi, \pi)$ if we define $\phi(0) = 0$.

Integrating by parts we get
\[
J_m(\eta) = -\frac{\phi(\eta)}{2m+1} \cos (2m+1)\eta + \frac{1}{2m+1} \int_0^\eta \phi'(t)\cos (2m+1)t \, dt,
\]
from which it is easily seen that
\[
|J_m(\eta)| < \frac{M_0 + \pi M_1}{2m+1},
\]
where $M_0$ and $M_1$ are the maxima of $|\phi|$ and $|\phi'|$ on the interval $[0, \pi - \delta/2]$ respectively. Thus $J_m(\eta)$ converges uniformly to $0$ as $m \to \infty$ on $[0, \pi - \delta/2]$.

We next deal with the integral $I_m(\eta)$. By the substitution $s = (2m+1)t$, we have
\[
I_m(\eta) = \int_0^{(2m+1)\eta} \frac{\sin s}{s} \, ds.
\]
Since $\frac{\pi}{2} = I_m(\pi/2) + J_m(\pi/2)$, we have for any $x > \pi$
\[
\left| \frac{\pi}{2} - \int_0^x \frac{\sin s}{s} \, ds \right| \leq \left| J_m\left(\frac{\pi}{2}\right) \right| + \int_{(2m+1)\pi/2}^x \frac{ds}{s},
\]
where $m_x$ is the largest integer satisfying $(2m+1)\pi \leq 2x$. Since the right-hand side converges to $0$ as $x \to \infty$, the improper integral
\[
\int_0^\infty \frac{\sin s}{s} \, ds
\]
exists and equals to $\pi/2$. Therefore
\[
\left| \frac{\omega}{2} + \sum_{n=1}^m \frac{\sin n\omega}{n} \right| \leq \left| J_m(\eta) \right| + \left| I_m(\eta) - \frac{\pi}{2} \right|
= \left| J_m(\eta) \right| + \left| \int_{(2m+1)\eta}^\infty \frac{\sin s}{s} \, ds \right|.
\]
The right-hand side clearly converges to $0$ uniformly in $\eta \in [\delta/2, \pi - \delta/2]$ as $m \to \infty$."
Series of Functions,"Suppose that \( f \in C^1(0,1) \) and \( \int_0^1 |f(x)| dx \) converges. Show that the Fourier series
\[
\frac{a_0}{2} + \sum_{n=1}^\infty \left( a_n \cos 2n\pi x + b_n \sin 2n\pi x \right)
\]
converges to \( f(x) \) in the interval \( (0,1) \).","For an arbitrary fixed $x \in (0, 1)$, let $s_n(x)$ be the $n$th partial sum of the Fourier series. Then we have
\[
s_n(x) = \frac{a_0}{2} + \sum_{k=1}^n \left(a_k \cos 2k\pi x + b_k \sin 2k\pi x\right)
= \int_0^1 f(t) \left(1 + 2 \sum_{k=1}^n \cos 2k\pi(t-x) \right) dt.
\]

Now, as we have already seen in \textbf{Solution 7.10}, the trigonometric sum in the big parentheses can be expressed in the ratio of sines; hence we obtain
\[
s_n(x) = \int_{-x}^{1-x} f(x+y) \frac{\sin (2n+1)\pi y}{\sin \pi y} \, dy.
\]

If $f(x) = 1$, then obviously $a_0 = 1$ and $a_n = b_n = 0$ for all $n \geq 1$; hence
\[
1 = \int_{-x}^{1-x} \frac{\sin (2n+1)\pi y}{\sin \pi y} \, dy.
\]

Therefore, we have
\[
s_n(x) - f(x) = \int_{-x}^{1-x} \varphi_x(y) \sin (2n+1)\pi y \, dy,
\]
where
\[
\varphi_x(y) = \frac{f(x+y) - f(x)}{\sin \pi y}.
\]

Clearly, $\varphi_x \in C(-x, 1-x)$ and the improper integral
\[
\int_{-x}^{1-x} |\varphi_x(y)| \, dy
\]
converges, since $f \in C^1(0, 1)$ and $\int_0^1 |f(x)| dx$ converges. Thus it follows from the remark after \textbf{Solution 5.1} that
\[
\lim_{n \to \infty} s_n(x) = f(x),
\]
in view of
\[
\int_{-x}^{1-x} \sin 2\pi y \, dy = \int_{-x}^{1-x} \cos 2\pi y \, dy = 0.
\]"
Series of Functions,"Let
\[
\sum_{n=1}^\infty a_n x^n
\]
be the Taylor series about \( x = 0 \) of the algebraic function
\[
f(x) = \frac{1 - \sqrt{1 - 4x}}{2}.
\]
Show that each \( a_n \) is a positive integer and that \( a_n \) is odd if and only if \( n \) is a power of 2.","It follows from
\[
\left( 1 - 2 \sum_{n=1}^\infty a_n x^n \right)^2 = 1 - 4x
\]
that $a_1 = 1$ and the recursion formula
\[
a_{n+1} = a_1 a_n + a_2 a_{n-1} + \cdots + a_n a_1
\]
holds for any positive integer $n$. This implies immediately that every $a_n$ is a positive integer; therefore
\[
a_{2k+1} = 2(a_1 a_{2k} + \cdots + a_k a_{k+1})
\]
is an even integer for any positive integer $k$.

Suppose now that there exists a non-negative integer $\ell$ such that $a_n$ is even for every $n = 2^\ell(2k+1)$ with $k \geq 1$. Then
\[
a_{2n} = 2(a_1 a_{2n-1} + a_2 a_{2n-2} + \cdots + a_{n-1} a_{n+1}) + a_n^2
\]
implies that $a_{2n}$ is also even for every
\[
n = 2^\ell(2k+1)
\]
with $k \geq 1$. Hence by induction every $a_n$ is shown to be even except for the case in which $n$ is a power of $2$. However, if $n$ is a power of $2$, we can similarly show that $a_n$ is odd, since $a_1$ is odd."
Approximation by Polynomials,"For any $f \in C[0, 1]$, the polynomial of degree $n$ defined by
\[
B_n(f; x) = \sum_{k=0}^n f\left(\frac{k}{n}\right) \binom{n}{k} x^k (1-x)^{n-k}
\]
is called the Bernstein polynomial. Show then that $B_n(f; x)$ converges to $f(x)$ uniformly on the interval $[0, 1]$.","For any positive $\epsilon$ it follows from the uniform continuity of $f$ that there is a positive $\delta$ such that $\lvert f(x) - f(y) \rvert < \epsilon$ for any $x$ and $y$ in the interval $[0, 1]$ with $\lvert x - y \rvert < \delta$. Let $M$ be the maximum of $\lvert f(x) \rvert$ on $[0, 1]$ and let $\{d_n\}$ be any monotone sequence of positive integers diverging to $\infty$ and satisfying $d_n = o(n)$ as $n \to \infty$. We divide the difference into two parts as follows:
\[
B_n(f; x) - f(x) = \sum_{k=0}^n \left( f\left(\frac{k}{n}\right) - f(x) \right) \binom{n}{k} x^k (1 - x)^{n-k} = S_0(x) + S_1(x),
\]
where in $S_0(x)$ the summation runs through the values of $k \in [0, n]$ for which $\lvert x - k/n \rvert \leq d_n / n$ and in $S_1(x)$ the summation runs through the remaining values of $k$.

Now we consider all sufficiently large $n$ satisfying $d_n / n < \delta$. For the sum $S_0(x)$, using $\lvert f(x) - f(k/n) \rvert < \epsilon$ we get
\[
\lvert S_0(x) \rvert \leq \epsilon \sum_{k=0}^n \binom{n}{k} x^k (1 - x)^{n-k} = \epsilon.
\]
On the other hand, for the sum $S_1(x)$, using $\lvert j - nx \rvert > d_n$ we obtain
\[
\lvert S_1(x) \rvert < 2M \sum_{k=0}^n \frac{(j - nx)^2}{d_n^2} \binom{n}{k} x^k (1 - x)^{n-k} 
= \frac{2M}{d_n^2} \left( n^2x^2 + nx(1 - x) - 2n^2x^2 + n^2x^2 \right) 
= \frac{2M}{d_n^2} nx(1 - x) \leq \frac{nM}{2d_n^2},
\]
where we used the fact that $B_n(1; x) = 1$, $B_n(x; x) = x$, and 
\[
B_n(x^2; x) = x^2 + \frac{x(1 - x)}{n}.
\]

We then take $d_n = \lfloor n^{2/3} \rfloor$ so that $d_n / \sqrt{n}$ diverges as $n \to \infty$. Hence we have $\lvert S_0(x) + S_1(x) \rvert < 2\epsilon$ for all sufficiently large $n$."
Approximation by Polynomials,"Show that there exists a sequence of polynomials with integral coefficients converging to $f \in C[0, 1]$ uniformly if $f(0) = f(1) = 0$.","The Bernstein polynomial for \(f\) defined in \textbf{Problem 8.1} is
\[
B_n(f; x) = \sum_{k=1}^{n-1} f\left(\frac{k}{n}\right) \binom{n}{k} x^k (1-x)^{n-k}.
\]

Since \(f(0) = f(1) = 0\), if \(n = p\) is a prime number, then the binomial coefficient \(\binom{p}{k}\) is clearly a multiple of \(p\) for every integer \(k \in [1, p)\). Hence,
\[
\widetilde{B}_p(f; x) = \sum_{k=1}^{p-1} \frac{1}{p} \binom{p}{k} \left\lfloor \frac{p}{k} \right\rfloor f\left(\frac{k}{p}\right) x^k (1-x)^{p-k}
\]
is a polynomial with integral coefficients where \(\lfloor x \rfloor\) denotes the integral part of \(x\). 

Since we have
\[
\left| B_p(f; x) - \widetilde{B}_p(f; x) \right| \leq \frac{1}{p} \sum_{k=1}^{p-1} \binom{p}{k} x^k (1-x)^{p-k} < \frac{1}{p},
\]
it is clear that \(\widetilde{B}_p(f; x)\) converges also to \(f(x)\) uniformly as \(p \to \infty\).
"
Approximation by Polynomials,Deduce the approximation theorem by trigonometric polynomials from Weierstrass' approximation theorem by polynomials.,"For any continuous periodic function with period $2\pi$, we write $f(x) = f_+(x) + f_-(x)$ where 
\[
f_{\pm}(x) = \frac{f(x) \pm f(-x)}{2}
\]
respectively. $f_{\pm}$ are also continuous periodic functions with the same period satisfying $f_+(-x) = f_+(x)$ and $f_-(x) + f_-(k\pi) = 0$. Note that $f_-(k\pi) = 0$ for any integer $k$.

For any $\epsilon > 0$ we can take a continuous odd function $\phi(x)$ with period $2\pi$ such that $|f_-(x) - \phi(x)| < \epsilon$ for any $x \in \mathbb{R}$ and that $\phi(x)$ vanishes on every point of some small neighborhoods of the points $k\pi$. Since $x = \arccos y$ maps the interval $[-1,1]$ onto $[0,\pi]$ homeomorphically, the functions 
\[
f_+(\arccos y) \quad \text{and} \quad \frac{\phi(\arccos y)}{\sin(\arccos y)}
\]
are continuous on the interval $[-1,1]$. Thus applying Weierstrass' approximation theorem to these functions, we can find certain polynomials $P(y)$ and $Q(y)$ satisfying
\[
|f_+(x) - P(\cos x)| < \epsilon \quad \text{and} \quad \left|\phi(x) - (\sin x) Q(\cos x)\right| < \epsilon
\]
for any $0 \leq x \leq \pi$. Moreover the inequalities hold for any $x \in \mathbb{R}$ by the evenness of $f_+(x)$ and $P(\cos x)$, by the oddness of $\phi(x)$ and $(\sin x) Q(\cos x)$ and, of course,

by the periodicity of these functions. We therefore obtain 
\[
|f(x) - P(\cos x) - (\sin x) Q(\cos x)| \leq |f_+(x) - P(\cos x)| + |f_-(x) - \phi(x)| + |\phi(x) - (\sin x)Q(\cos x)|
< 3\epsilon
\]
for any $x \in \mathbb{R}$. Finally it is easily verified that each $\cos^k x$ can be written as a linear combination of $1, \cos x, \ldots, \cos kx$ and that each $(\sin x) \cos^k x$ can be expressed as a linear combination of $\sin x, \sin 2x, \ldots, \sin kx$. This completes the proof."
Approximation by Polynomials,"For any $f \in C^1[0,1]$, show that $B_n'(f;x)$ converges to $f'(x)$ uniformly on the interval $[0,1]$, where $B_n(f;x)$ is the Bernstein polynomial.","Since
\[
B_n'(f;x) = \sum_{k=0}^n f\left(\frac{k}{n}\right) \binom{n}{k} \left( kx^{k-1}(1-x)^{n-k} - (n-k)x^k(1-x)^{n-k-1} \right),
\]
\[
= n \sum_{k=0}^{n-1} \left( f\left(\frac{k+1}{n}\right) - f\left(\frac{k}{n}\right) \right) \binom{n-1}{k} x^k (1-x)^{n-k-1},
\]
it follows from the mean value theorem that
\[
B_n'(f;x) = n \sum_{k=0}^{n-1} f'\left(\frac{k+\xi_k}{n}\right) \binom{n-1}{k} x^k (1-x)^{n-k-1},
\]
for some $\xi_k$ in the interval $(0,1)$. Since for any $\epsilon > 0$ there exists an integer $N$ such that $|f'(x) - f'(y)| < \epsilon$ for all $x$ and $y$ in $[0,1]$ with $|x-y| \leq 1/N$, we have
\[
\left| B_n'(f;x) - B_{n-1}'(f;x) \right| \leq \sum_{k=0}^{n-1} \left| f'\left(\frac{k+\xi_k}{n}\right) - f'\left(\frac{k}{n}\right) \right| \binom{n-1}{k} x^k (1-x)^{n-k-1},
\]
\[
< \epsilon \sum_{k=0}^{n-1} \binom{n-1}{k} x^k (1-x)^{n-k-1} = \epsilon,
\]
for all integers $n > N$."
Approximation by Polynomials,"Show that $f \in C[0,1]$ satisfies 
\[
\int_0^1 x^n f(x) \, dx = 0
\]
for all non-negative integers $n$ if and only if $f(x)$ vanishes everywhere on the interval $[0,1]$.","For any $\epsilon > 0$, there exists a polynomial $P(x)$ satisfying $\lvert f(x) - P(x) \rvert < \epsilon$ on the interval $[0,1]$ by Weierstrass' approximation theorem. Letting $M$ be the maximum of $\lvert f(x) \rvert$ on $[0,1]$, we get
\[
\int_0^1 f^2(x) \, dx = \int_0^1 (f(x) - P(x)) f(x) \, dx + \int_0^1 P(x) f(x) \, dx,
\]
\[
\leq \int_0^1 \lvert f(x) - P(x) \rvert \cdot \lvert f(x) \rvert \, dx < \epsilon M.
\]
Since $\epsilon$ is arbitrary, we have
\[
\int_0^1 f^2(x) \, dx = 0;
\]
hence $f(x)$ vanishes everywhere."
Approximation by Polynomials,"For all non-negative integers $n$, show that
\[
\int_0^\infty x^n (\sin x^{1/4}) \exp(-x^{1/4}) \, dx = 0.
\]","For brevity, put
\[
I_n = \int_0^\infty x^n e^{-x} \sin x \, dx \quad \text{and} \quad J_n = \int_0^\infty x^n e^{-x} \cos x \, dx
\]
for any non-negative integer $n$. By the substitution $t = x^{1/4}$, the given integral in the problem is equal to $4 I_{4n+3}$. By partial integration, we easily get
\[
\begin{cases}
I_n = \frac{n}{2} \left( I_{n-1} + J_{n-1} \right), \\
J_n = \frac{n}{2} \left( J_{n-1} - I_{n-1} \right),
\end{cases}
\]
for any positive integer $n$. Solving these recursion formulae with the initial condition $I_0 = J_0 = 1/2$, we obtain $I_n = 0$ for any integer $n$ satisfying $n \equiv 3 \, (\text{mod} \, 4)$."
Approximation by Polynomials,"Put $a_0 = 0$ and let $\{a_n\}_{n \geq 1}$ be a sequence of distinct positive numbers such that
\[
\frac{1}{a_1} + \frac{1}{a_2} + \cdots + \frac{1}{a_n} + \cdots
\]
diverges. Then show that $f \in C[0, 1]$ satisfies
\[
\int_0^1 x^{a_n} f(x) \, dx = 0
\]
for all $n \geq 0$ if and only if $f(x)$ vanishes everywhere on the interval $[0, 1]$.","Let $m$ be any positive integer satisfying $m \neq a_n$ for all $n \geq 0$. We first consider the definite integral
\[
I_n = \int_0^1 \big(x^m - c_0 - c_1 x^{a_1} - c_2 x^{a_2} - \cdots - c_n x^{a_n}\big)^2 dx
\]
for any positive integer $n$. Obviously $I_n$ is a polynomial in $c_0, c_1, \ldots, c_n$ of degree 2 and attains its minimum $I_n^*$ at some point $(s_0, s_1, \ldots, s_n) \in \mathbb{R}^{n+1}$, which is a unique solution of the system of $n+1$ linear equations:
\[
\sum_{j=0}^n \frac{s_j}{a_i + a_j + 1} = \frac{1}{m + a_i + 1} \quad \text{for } 0 \leq i \leq n.
\]
Here the coefficient matrix
\[
A = \bigg(\frac{1}{a_i + a_j + 1}\bigg)_{0 \leq i, j \leq n} \in M_{n+1}(\mathbb{R})
\]
is symmetric and the determinant can be written explicitly as
\[
\det A = \frac{\prod_{0 \leq i < j \leq n} (a_i - a_j)^2}{\prod_{0 \leq i, j \leq n} (a_i + a_j + 1)}.
\]
By using the Cauchy determinant, it follows that
\[
I_n^* = \frac{1}{2m + 1} - \frac{s_0}{m + a_0 + 1} - \frac{s_1}{m + a_1 + 1} - \cdots - \frac{s_n}{m + a_n + 1}.
\]
Combining the above equations, we have
\[
A
\begin{bmatrix}
s_0 \\ \vdots \\ s_n
\end{bmatrix}
=
\begin{bmatrix}
t \mathbf{a} \\
1
\end{bmatrix}
\quad \text{where } \mathbf{a} = \bigg(\frac{1}{m + a_0 + 1}, \ldots, \frac{1}{m + a_n + 1}\bigg) \in \mathbb{R}^{n+1}.
\]
Therefore we get
\[
I_n^* = \frac{\det B}{\det A},
\]
where
\[
B =
\begin{bmatrix}
A & t\mathbf{a} \\
\mathbf{a} & \frac{1}{2m + 1}
\end{bmatrix} \in M_{n+2}(\mathbb{R}),
\]
is again symmetric and $\det B$ can be obtained from the Cauchy determinant by substituting $a_{n+1}$ by $m$ formally. We thus have
\[
I_n^* = \frac{\det B}{\det A} = \frac{1}{2m + 1} \prod_{k=0}^n \bigg(\frac{a_k - m}{a_k + m + 1}\bigg)^2.
\]
Since
\[
\log \frac{|a_k - m|}{a_k + m + 1} = \log \bigg(1 - \frac{2m + 1}{a_k + m + 1}\bigg) \leq -\frac{2m + 1}{a_k + m + 1} \leq -\frac{m}{a_k},
\]
for any $k$ satisfying $a_k > m$ and $\sum_{k=1}^\infty 1/a_k = \infty$ by the assumption, we infer that $I_n^* \to 0$ as $n \to \infty$.

For any $\epsilon > 0$ we can take a sufficiently large $n$ and $(c_0, c_1, \ldots, c_n) \in \mathbb{R}^{n+1}$ such that
\[
\int_0^1 \big(x^m - c_0 - c_1 x^{a_1} - c_2 x^{a_2} - \cdots - c_n x^{a_n}\big)^2 dx < \epsilon.
\]
By the Cauchy-Schwarz inequality, we obtain
\[
\bigg(\int_0^1 x^m f(x) dx\bigg)^2 = \bigg(\int_0^1 \big(x^m - c_0 - c_1 x^{a_1} - c_2 x^{a_2} - \cdots - c_n x^{a_n}\big) f(x) dx\bigg)^2
\leq M \int_0^1 \big(x^m - c_0 - c_1 x^{a_1} - c_2 x^{a_2} - \cdots - c_n x^{a_n}\big)^2 dx < \epsilon M,
\]
where $M = \int_0^1 f^2(x) dx$. Since $\epsilon$ is arbitrary,
\[
\int_0^1 x^m f(x) dx = 0 \quad \text{for any positive integer } m \text{ satisfying } m \neq a_n \text{ for all } n \geq 0.
\]
Hence $\int_0^1 x^m f(x) dx = 0$ for all non-negative integers $n$ and $f(x)$ vanishes everywhere on the interval $[0, 1]$ by \textbf{Problem 8.5}.
\hfill"
Convex Functions,"Suppose that $f(x)$ is convex on an interval $I$. Show that
\[
f\left(\frac{x_1 + x_2 + \cdots + x_n}{n}\right) \leq \frac{f(x_1) + f(x_2) + \cdots + f(x_n)}{n}
\]
for arbitrary $n$ points $x_1, x_2, \ldots, x_n$ in $I$.","By \( m \) times applications of convexity for \( f \), we easily get
\[
f\left(\frac{x_1 + x_2 + \cdots + x_{2^m}}{2^m}\right) \leq \frac{f(x_1) + f(x_2) + \cdots + f(x_{2^m})}{2^m}
\]
for any \( 2^m \) points \( x_1, \ldots, x_{2^m} \) in \( I \). For any integer \( n \geq 3 \), we choose \( m \) satisfying \( n < 2^m \). Now adding to \( x_1, \ldots, x_n \) the new \( 2^m - n \) points
\[
x_{n+1} = \cdots = x_{2^m} = \frac{x_1 + x_2 + \cdots + x_{2^m}}{n}
\]
in \( I \), we have
\[
2^m f(y) \leq f(x_1) + \cdots + f(x_n) + (2^m - n) f\left(\frac{x_1 + x_2 + \cdots + x_n}{n}\right)
\]
where
\[
y = \frac{1}{2^m} \left(x_1 + x_2 + \cdots + x_n + (2^m - n) \frac{x_1 + x_2 + \cdots + x_n}{n} \right)
= \frac{x_1 + x_2 + \cdots + x_n}{n};
\]
namely
\[
f\left(\frac{x_1 + x_2 + \cdots + x_n}{n}\right) \leq \frac{f(x_1) + f(x_2) + \cdots + f(x_n)}{n}.
\]"
Convex Functions,Show that $f(x)$ is convex on an interval $I$ if and only if $e^{\lambda f(x)}$ is convex on $I$ for any positive $\lambda$.,"Suppose first that $f(x)$ is convex on the interval $I$. Since $e^{\lambda x}$ is convex and monotone increasing on $\mathbb{R}$, we have
\[
\exp\left(\lambda f\left(\frac{x_1 + x_2}{2}\right)\right) \leq \exp\left(\frac{\lambda f(x_1) + f(x_2)}{2}\right) \leq \frac{e^{\lambda f(x_1)} + e^{\lambda f(x_2)}}{2}
\]
for any $x_1$ and $x_2$ in $I$. Hence $e^{\lambda f(x)}$ is convex on $I$.

Conversely, suppose that $e^{\lambda f(x)}$ is convex on $I$ for any positive $\lambda$. The asymptotic expansions as $\lambda \to 0^+$ of both sides of the inequality
\[
\exp\left(\lambda f\left(\frac{x_1 + x_2}{2}\right)\right) \leq \frac{e^{\lambda f(x_1)} + e^{\lambda f(x_2)}}{2}
\]
give
\[
1 + \lambda f\left(\frac{x_1 + x_2}{2}\right) + O(\lambda^2) \leq 1 + \lambda \frac{f(x_1) + f(x_2)}{2} + O(\lambda^2),
\]
for any $x_1$ and $x_2$ in $I$, which implies the convexity of $f(x)$ on $I$. "
Convex Functions,"Suppose that $f(x)$ is convex and bounded above on an open interval $(a, b)$. Show then that $f(x)$ is continuous on $(a, b)$.","Suppose that $f(x) < K$ for some constant $K$. For an arbitrary fixed $y$ in the interval $(a, b)$ and any positive integer $n$, the points $y \pm n\delta$ belong to $(a, b)$ for all sufficiently small positive $\delta$. Of course $\delta$ depends on $n$. Applying the inequality described in \textbf{Problem 9.1} to the $n$ points $x_1 = y \pm n\delta, x_2 = \cdots = x_n = y$, we get
\[
f(y \pm \delta) \leq \frac{f(y \pm n\delta) + (n-1)f(y)}{n}
\]
respectively. Hence we have
\[
f(y + \delta) - f(y) \leq \frac{f(y + n\delta) - f(y)}{n} \leq \frac{K - f(y)}{n}
\]
and
\[
f(y) - f(y - \delta) \geq \frac{f(y) - f(y - n\delta)}{n} \geq \frac{f(y) - K}{n}.
\]

Since $f(y) - f(y - \delta) \leq f(y + \delta) - f(y)$ and $n$ is arbitrary, these mean that $f(x)$ is continuous at the point $y$. "
Convex Functions,"Suppose that $f(x)$ is convex and continuous on an interval $I$. Show that 
\[
f\left( \frac{\lambda_1 x_1 + \cdots + \lambda_n x_n}{\lambda_1 + \cdots + \lambda_n} \right) 
\leq 
\frac{\lambda_1 f(x_1) + \cdots + \lambda_n f(x_n)}{\lambda_1 + \cdots + \lambda_n}
\]
for any $n$ points $x_1, \dots, x_n$ in $I$ and any positive numbers $\lambda_1, \dots, \lambda_n$.","By the inequality in \textbf{Problem 9.1}, it is easily seen that
\[
f\left(\frac{k_1x_1 + \cdots + k_nx_n}{k_1 + \cdots + k_n}\right) \leq \frac{k_1f(x_1) + \cdots + k_nf(x_n)}{k_1 + \cdots + k_n}
\]
for any points $x_1, \dots, x_n$ in $I$ and any positive integers $k_1, \dots, k_n$. For any sufficiently large integer $N$, we take $k_j = \lfloor \lambda_j N / (\lambda_1 + \cdots + \lambda_n) \rfloor$ for each $1 \leq j \leq n$. Since
\[
\frac{k_j}{k_1 + \cdots + k_n} \to \frac{\lambda_j}{\lambda_1 + \cdots + \lambda_n}
\]
as $N \to \infty$, the required inequality follows from the continuity of $f$."
Convex Functions,"Suppose that $g \in C[a,b]$ and $p(x)$ is a non-negative continuous function defined on $[a,b]$ satisfying $\sigma = \int_a^b p(x) \, dx > 0$. Let $m$ and $M$ be the minimum and maximum of the function $g(x)$ on $[a,b]$, respectively. Suppose further that $f$ is a continuous convex function defined on $[m, M]$. Show then that
\[
f\left( \frac{1}{\sigma} \int_a^b g(x)p(x) \, dx \right) \leq \frac{1}{\sigma} \int_a^b f(g(x))p(x) \, dx.
\]","We divide the interval $[a, b]$ into $n$ equal parts and put
\[
\lambda_k = \int_{t_{k-1}}^{t_k} p(x) \, dx \geq 0;
\]
for any subinterval $[t_{k-1}, t_k]$ so that $\sigma = \sum_{k=1}^n \lambda_k$. It follows from the first mean value theorem that
\[
\int_{t_{k-1}}^{t_k} g(x)p(x) \, dx = \lambda_k g(\xi_k)
\]
for some $\xi_k \in (t_{k-1}, t_k)$. Applying the inequality in \textbf{Problem 9.4} to $n$ points $x_k = g(\xi_k)$ in $[m, M]$, we obtain
\[
f\left(\frac{1}{\sigma} \int_a^b g(x)p(x) \, dx\right) = f\left(\frac{1}{\sigma} \sum_{k=1}^n \lambda_k g(\xi_k)\right)
\]
\[
\leq \frac{1}{\sigma} \sum_{k=1}^n \lambda_k f(g(\xi_k))
= \frac{b - a}{\sigma n} \sum_{k=1}^n f(g(\xi_k)) p(\eta_k)
\]
for some $\eta_k \in (t_{k-1}, t_k)$. By the uniform continuity of $p(x)$, the difference between the expression on the right-hand side and one with $p(\xi_k)$ replaced by $p(\eta_k)$ is sufficiently small whenever $n$ is sufficiently large. Therefore the right-hand side converges to
\[
\frac{1}{\sigma} \int_a^b f(g(x))p(x) \, dx
\]
as $n \to \infty$."
Convex Functions,Show that any continuous convex function $f(x)$ on an open interval $I$ possesses a finite derivative except for at most countable points.,"Let \( f'_+(x) \) and \( f'_-(x) \) be the right- and left-hand derivatives of \( f \) at \( x \) respectively. First we will prove that \( f'_\pm(x) \) exist at every point \( x \) in the open interval \( I \). For brevity let \( \Delta(x,y) \) denote the difference quotient
\[
\Delta(x, y) = \frac{f(x) - f(y)}{x - y}
\]
for any \( x \neq y \) in \( I \). Let \( x < y < z \) be arbitrary three points in \( I \). Applying the inequality stated in \textbf{Problem 9.4} with \( \lambda_1 = z-y, x_1 = x \) and \( \lambda_2 = y-x, x_2 = z \),

we get \( y = (\lambda_1 x_1 + \lambda_2 x_2)/(\lambda_1 + \lambda_2) \) and
\[
f(y) \leq \frac{z-y}{z-x} f(x) + \frac{y-x}{z-x} f(z).
\]
Thus \( \Delta(x, y) \leq \Delta(y, z) \). Note that the above inequality can also be written as \( \Delta(x, y) \leq \Delta(x, z) \) or as \( \Delta(x, z) \leq \Delta(y, z) \). In particular, the quotients \( \Delta(x-h, x) \) and \( \Delta(x, x+h) \) are monotone increasing with respect to \( h > 0 \) and \( \Delta(x-h, x) \leq \Delta(x, x+h) \) holds for any \( h > 0 \) satisfying \( x \pm h \in I \). This means that both \( f'_+(x) \) and \( f'_-(x) \) certainly exist and satisfy \( f'_-(x) \leq f'_+(x) \).

Next for any points \( x < y \) in \( I \) we take a sufficiently small \( h > 0 \) satisfying \( x + h < y - h \). Then
\[
\Delta(x, x+h) \leq \Delta(x+h, y-h) \leq \Delta(y-h, y);
\]
hence, letting \( h \to 0^+ \) we obtain \( f'_+(x) \leq f'_-(y) \). Therefore if \( f(x) \) does not possess a finite differential coefficient at \( x_0 \), then it follows that \( f'_-(x_0) < f'_+(x_0) \). Moreover if we assign the point \( x_0 \) to the open interval \( (f'_-(x_0), f'_+(x_0)) \), then such intervals are disjoint each other. Therefore we can enumerate all such open intervals by labeling them, for example, in such a way that we count ones contained in \( (-n, n) \) and having the length \( > 1/n \) for each positive integer \( n \). \(\square\)"
Convex Functions,"Show that $f \in C[a, b]$ is convex if and only if
\[
\frac{1}{t - s} \int_s^t f(x) \, dx \leq \frac{f(s) + f(t)}{2}
\]
for any $s \neq t$ in $[a, b]$.","First assume that a continuous function \( f(x) \) is convex on the interval \([a,b]\). We divide the subinterval \([s, t]\) into \(n\) equal parts and put
\[
x_k = \frac{n-k}{n}s + \frac{k}{n}t
\]
for \( 0 \leq k \leq n \). It follows from the inequality in \textbf{Problem 9.1} that
\[
f(x_k) \leq \frac{n-k}{n}f(s) + \frac{k}{n}f(t).
\]
Therefore we have
\[
\frac{1}{t-s} \int_s^t f(x) \, dx = \lim_{n \to \infty} \frac{1}{n} \sum_{k=0}^n f(x_k) \leq \limsup_{n \to \infty} \frac{n(n+1)}{2n^2}(f(s) + f(t)) = \frac{f(s) + f(t)}{2}.
\]

Conversely assume that
\[
\frac{1}{t-s} \int_s^t f(x) \, dx \leq \frac{f(s) + f(t)}{2}
\]
for any \( s \neq t \) in \([a,b]\). Suppose, on the contrary, that there are two points \( s < t \) in the interval \([a, b]\) satisfying
\[
f\left(\frac{s+t}{2}\right) > \frac{f(s) + f(t)}{2}.
\]
Then we consider the set
\[
E = \left\{ x \in [s,t]; f(x) > f(s) + \frac{f(t) - f(s)}{t-s}(x-s) \right\}.
\]
The set \(E\) is clearly open by the continuity of \(f\) and \(E \neq \emptyset\) since it contains the point \((s+t)/2\). Note that \(E\) is the set of points on the interval \([s,t]\) at which the graph of \(f(x)\) is situated in the upper side of the straight line through the two points \((s, f(s))\) and \((t, f(t))\). Let \((u, v)\) be the connected component of \(E\) containing the point \((s+t)/2\). Since the end points \((u, f(u))\) and \((v, f(v))\) must be on that line, we have
\[
\begin{cases}
f(u) = f(s) + \frac{f(t)-f(s)}{t-s}(u-s), \\
f(v) = f(s) + \frac{f(t)-f(s)}{t-s}(v-s),
\end{cases}
\]
which imply that
\[
\frac{f(u) + f(v)}{2} = f(s) + \frac{f(t) - f(s)}{t-s}\left(\frac{u+v}{2}-s\right).
\]
Therefore
\[
\frac{1}{v-u} \int_u^v f(x) \, dx > \frac{1}{v-u} \int_u^v \left(f(s) + \frac{f(t)-f(s)}{t-s}(x-s)\right) \, dx = f(s) + \frac{f(t)-f(s)}{t-s}\left(\frac{u+v}{2}-s\right),
\]
contrary to the assumption."
Convex Functions,"Let $I$ be a closed interval of the form either $[0, a]$ or $[0, \infty)$. Suppose that $f \in C(I)$ satisfies $f(0) = 0$. Show then that $f$ is convex if and only if
\[
\sum_{k=1}^n (-1)^{k-1} f(x_k) \geq f\left( \sum_{k=1}^n (-1)^{k-1} x_k \right)
\]
for any integer $n \geq 2$ and any $n$ points $x_1 \geq x_2 \geq \cdots \geq x_{n-1} \geq x_n$ in the interval $I$.","Suppose first that $f$ is convex on the closed interval $I$. Let $x_1 > x_2 > x_3$ be arbitrary three points in $I$ and define a positive number $\lambda$ with $x_2 = \lambda x_1 + (1-\lambda)x_3$. Since $f$ is convex, it follows from \textbf{Problem 9.4} that
\[
f(x_2) = f(\lambda x_1 + (1-\lambda)x_3) \leq \lambda f(x_1) + (1-\lambda)f(x_3)
\]
and
\[
f((1-x_2+x_3)) = f((1-\lambda)x_1 + \lambda x_3) \leq (1-\lambda)f(x_1) + \lambda f(x_3);
\]
therefore we have
\[
f(x_2) + f(x_1-x_2+x_3) \leq f(x_1) + f(x_3),
\]
which is also valid for any $x_1 \geq x_2 \geq x_3$ in $I$ by the continuity of $f$. If we take $x_3 = 0$, then clearly $f(x_1-x_2) \leq f(x_1)-f(x_2)$ by virtue of $f(0) = 0$. We thus have the inequality in the problem for the cases $n=2$ and $3$. Suppose now the inequality holds for $n=m+2$. Then for any $m+2$ points $x_1 \geq x_2 \geq \cdots \geq x_{m+2}$ in the interval $I$,
\[
\sum_{k=1}^{m+2} (-1)^{k-1} f(x_k) \geq f(x_1)-f(x_2)+f \left( \sum_{k=3}^{m+2} (-1)^{k-1} x_k \right),
\]
which means that the inequality holds for $n=m+2$; therefore for every $n \geq 2$.

Conversely suppose the case $n=3$:
\[
f(x_2)+f(x_1-x_2+x_3) \leq f(x_1)+f(x_3)
\]
for any points $x_1 \geq x_2 \geq x_3$ in $I$. By taking $x_2 = (x_1+x_3)/2$ we get
\[
f\left(\frac{x_1+x_3}{2}\right) \leq \frac{f(x_1)+f(x_3)}{2};
\]
hence $f$ is convex on $I$."
Convex Functions,"Let $s > -1$ be a real number. Suppose that $f \in C[0, \infty)$ is a convex function having the piecewise continuous derivative $f'(x)$ and satisfying $f(0) \geq 0$. Suppose further that $f'(0+)$ exists when $f(0) = 0$. Then show that
\[
\int_0^\infty x^s \exp\left(-\frac{f(x)}{x}\right) dx \leq \int_0^\infty x^s \exp\left(-f'\left(\frac{x}{e}\right)\right) dx.
\]
Prove moreover that the constant $e$ in the denominator of the right-hand side cannot in general be replaced by any smaller number.","As is shown in \textbf{Solution 9.6}, the difference quotient
\[
\Delta(x+h, x) = \frac{f(x+h) - f(x)}{h}
\]
is monotone increasing for \(h > 0\); therefore we have \(\Delta(\alpha x, x) \geq \Delta(x+h, x)\) for any real numbers \(\alpha > 1\) and \(x > 0\) if \((\alpha - 1)x \geq h > 0\) is fulfilled. Letting \(h \to 0+\), we get \(\Delta(\alpha x, x) \geq f'(x)\) if the derivative exists; in other words,
\[
f(\alpha x) \geq f(x) + (\alpha - 1)x f'(x).
\]

For brevity put
\[
F(x) = x^s \exp\left(-\frac{f(x)}{x}\right) \quad \text{and} \quad G(x) = x^s \exp\left(-f'(x)\right).
\]

Note that the improper integral
\[
\int_{0}^{\infty} F(x) \, dx
\]
converges at \(x = 0\) for any \(s > -1\) when \(f(0) > 0\) or when \(f(0) = 0\) and \(f'(0+)\) exists, since \(f'(0+) \leq f(x)/x\) in the latter case. The convergence at \(x = \infty\) also follows. By the substitution \(x = \alpha t\), we have
\[
\int_{0}^{L} F(x) \, dx = \alpha^{s+1} \int_{0}^{L/\alpha} t^s \exp\left(-\frac{f(\alpha t)}{\alpha t}\right) dt
\]
\[
\leq \alpha^{s+1} \int_{0}^{L/\alpha} F^{1/\alpha}(t) G^{(\alpha - 1)/\alpha}(t) \, dt
\]
for any \(L > 0\). Applying Hölder's inequality, the right-hand side is less than or equal to
\[
\alpha^{s+1} \left( \int_{0}^{L/\alpha} F(x) \, dx \right)^{1/\alpha} \left( \int_{0}^{L/\alpha} G(x) \, dx \right)^{(\alpha - 1)/\alpha}
\]
and replacing \(L/\alpha\) by \(L\), we have
\[
\int_{0}^{L} F(x) \, dx \leq \phi^{s+1}(\alpha) \int_{0}^{L} G(x) \, dx
\]
where \(\phi(\alpha) = \alpha^{\alpha/(\alpha-1)}\) is strictly monotone increasing on the interval \((1, \infty)\). By letting \(\alpha \to 1+\), we get
\[
\int_{0}^{L} F(x) \, dx \leq e^{s+1} \int_{0}^{L} G(x) \, dx
\]
\[
= \int_{0}^{eL} t^s \exp\left(-f'\left(\frac{t}{e}\right)\right) dt.
\]
The desired inequality follows by letting \(L \to \infty\).

To see that the constant \(e\) is best possible, we take \(f(x) = x^\beta\) for any \(\beta > 1\). Then it is not hard to see that
\[
\int_{0}^{\infty} F(x) \, dx = \frac{1}{\beta - 1} \Gamma\left(\frac{s+1}{\beta - 1}\right)
\]
and
\[
\int_{0}^{\infty} G(x) \, dx = \frac{1}{(\beta - 1)\beta^{(s+1)/(\beta - 1)}} \Gamma\left(\frac{s+1}{\beta - 1}\right),
\]
where \(\Gamma(s)\) is the Gamma function. Hence the ratio of the two integrals converges to \(e^{s+1}\) as \(\beta \to 1+\)."
Convex Functions,Suppose that \( f(x) \) is twice differentiable in an open interval \( I \). Show then that \( f(x) \) is convex if and only if \( f''(x) \geq 0 \) on \( I \).,"Suppose that \( f(x) \) is convex on \( I \). We have already seen in the proof of \textbf{Problem 9.6} that \( f'_+(x) \leq f'_-(y) \) for any points \( x < y \) in \( I \). Since \( f(x) \) is twice differentiable, the derivative \( f'(x) \) is monotone increasing on \( I \), whence \( f''(x) \geq 0 \).

Conversely suppose that \( f''(x) \geq 0 \). Let \( x \) and \( y \) be any points in \( I \). By Taylor's formula centered at \( (x+y)/2 \) we obtain
\[
f(x) = f\left( \frac{x+y}{2} \right) + f'\left( \frac{x+y}{2} \right) \frac{x-y}{2} + \frac{f''(c)}{8} (x-y)^2
\]
and
\[
f(y) = f\left( \frac{x+y}{2} \right) + f'\left( \frac{x+y}{2} \right) \frac{y-x}{2} + \frac{f''(c')}{8} (x-y)^2
\]
for some \( c \) and \( c' \). Adding the two equalities we get
\[
f(x) + f(y) = 2f\left( \frac{x+y}{2} \right) + \frac{f''(c) + f''(c')}{8} (x-y)^2 \geq 2f\left( \frac{x+y}{2} \right).
\]
Hence \( f(x) \) is convex on \( I \)."
Convex Functions,"Suppose that \( f \in C^2[0, \infty) \) is convex and bounded. Show that the improper integral
\[
\int_0^\infty x f''(x) \, dx
\]
converges.","Suppose $\delta = f'(x_0) > 0$ for some $x_0 > 0$. Since $f'(x)$ is monotone increasing, we have $f'(x) \geq \delta$ for any $x \geq x_0$ and hence
\[
f(x) = f(x_0) + \int_{x_0}^x f'(t) \, dt \geq f(x_0) + \delta (x - x_0),
\]
contrary to the assumption that $f$ is bounded. Thus $f'(x) \leq 0$ for any $x > 0$ and it follows that $f(x)$ is monotone decreasing and converges to some $\lambda$ as $x \to \infty$. Therefore we have
\[
\lambda - f(x) = \int_x^\infty f'(t) \, dt
\]
and, in particular, $f'(x)$ converges to $0$ as $x \to \infty$. By the Cauchy criterion we have, for any $\epsilon > 0$,
\[
0 < -\int_\alpha^\beta f'(t) \, dt < \epsilon
\]
for all sufficiently large $\alpha$ and $\beta > \alpha$. Since $f'(x)$ is non-positive and monotone increasing, we obtain $(\beta - \alpha)|f'(\beta)| < \epsilon$; therefore
\[
\beta |f'(\beta)| < \epsilon + \alpha |f'(\beta)|.
\]
The right-hand side can be smaller than $2\epsilon$ if we take $\beta$ sufficiently large. Hence $x f'(x)$ converges to $0$ as $x \to \infty$. Thus
\[
\int_0^x t f''(t) \, dt = x f'(x) - f(x) + f(0)
\]
converges to $f(0) - \lambda$ as $x \to \infty$."