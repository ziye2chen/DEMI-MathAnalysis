{
    "problem": "Suppose that $f \\in C^2(0, \\infty)$ converges to $\\alpha$ as $x \\to \\infty$ and that \n\\[\nf''(x) + \\lambda f'(x)\n\\]\nis bounded above for some constant $\\lambda$. Then show that $f'(x)$ converges to $0$ as $x \\to \\infty$.",
    "problem_type": "Diï¬€erentiation",
    "solution": "The proof is substantially due to Hardy and Littlewood (1914). We use Taylorâ€™s formula with the integral remainder term\n\\[\nf(x+y) = f(x) + yf'(x) + y^2 \\int_0^1 (1-t)f''(x+yt) dt,\n\\]\nvalid for any \\(x > 1\\) and \\(|y| < 1\\). Now let us consider the integral on the right-hand side with \\(f''\\) replaced by \\(f'\\). By the mean value theorem there is a \\(\\xi_{x,y}\\) between \\(0\\) and \\(y\\) satisfying\n\\[\ny^2 \\int_0^1 (1-t)f'(x+yt) dt = \\int_x^{x+y} f(s) ds - yf(x) = y f(x+\\xi_{x,y}) - y f(x).\n\\]\nSince there exists a positive constant \\(K\\) satisfying \\(f''(x) + \\lambda f'(x) \\leq K\\), we have\n\\[\nf(x+y) - f(x) - y f'(x) + \\lambda y f(x+\\xi_{x,y}) - \\lambda y f(x) = y^2 \\int_0^1 (1-t) (f''(x+yt) + \\lambda f'(x+yt)) dt \\leq Ky^2 \\int_0^1 (1-t) dt = \\frac{K}{2}y^2.\n\\]\n\nFor the case in which \\(0 < y < 1\\) we get\n\\[\nf'(x) \\geq \\frac{f(x+y) - f(x)}{y} + \\lambda f(x+\\xi_{x,y}) - \\lambda f(x) - \\frac{K}{2}y.\n\\]\nMaking \\(x \\to \\infty\\) we thus have\n\\[\n\\liminf_{x \\to \\infty} f'(x) \\geq -\\frac{K}{2}y.\n\\]\nTherefore \\(\\liminf_{x \\to \\infty} f'(x)\\) must be \\(\\geq 0\\) because \\(y\\) is arbitrary.\n\nSimilarly, for the case in which \\(-1 < y < 0\\),\n\\[\nf'(x) \\leq \\frac{f(x) - f(x-|y|)}{|y|} + \\lambda f(x+\\xi_{x,y}) - \\lambda f(x) + \\frac{K}{2}|y|,\n\\]\nwhich implies that\n\\[\n\\limsup_{x \\to \\infty} f'(x) \\leq \\frac{K}{2}|y|.\n\\]\nSo that \\(\\limsup_{x \\to \\infty} f'(x)\\) is \\(\\leq 0\\) because \\(y\\) is arbitrary. Therefore \\(f'(x)\\) converges to \\(0\\) as \\(x \\to \\infty\\)."
}